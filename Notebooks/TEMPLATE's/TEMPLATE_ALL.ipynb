{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file: 01.CODE_TEMPLATE(ALL).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#TO-DO:\" data-toc-modified-id=\"TO-DO:-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>TO DO:</a></span></li></ul></li></ul></li><li><span><a href=\"#GENERAL-PYSPARK\" data-toc-modified-id=\"GENERAL-PYSPARK-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>GENERAL PYSPARK</a></span><ul class=\"toc-item\"><li><span><a href=\"#IMPORT-IMAGE\" data-toc-modified-id=\"IMPORT-IMAGE-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>IMPORT IMAGE</a></span></li><li><span><a href=\"#SAVE-CONTENTS-OF-THIS-CELL\" data-toc-modified-id=\"SAVE-CONTENTS-OF-THIS-CELL-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>SAVE CONTENTS OF THIS CELL</a></span></li><li><span><a href=\"#RUN-.PY-SCRIPT-FROM-JUPYTER\" data-toc-modified-id=\"RUN-.PY-SCRIPT-FROM-JUPYTER-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>RUN .PY SCRIPT FROM JUPYTER</a></span></li><li><span><a href=\"#GIT\" data-toc-modified-id=\"GIT-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>GIT</a></span></li><li><span><a href=\"#PUBLIC-DATASETS\" data-toc-modified-id=\"PUBLIC-DATASETS-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>PUBLIC DATASETS</a></span></li><li><span><a href=\"#OS-INFO\" data-toc-modified-id=\"OS-INFO-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>OS INFO</a></span></li></ul></li><li><span><a href=\"#PYSPARK-DATA-SCIENCE:\" data-toc-modified-id=\"PYSPARK-DATA-SCIENCE:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>PYSPARK DATA SCIENCE:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#GENERAL\" data-toc-modified-id=\"GENERAL-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>GENERAL</a></span></li><li><span><a href=\"#SELECT\" data-toc-modified-id=\"SELECT-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>SELECT</a></span></li><li><span><a href=\"#READ-FILE\" data-toc-modified-id=\"READ-FILE-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>READ FILE</a></span></li><li><span><a href=\"#SELECT-DISTICT\" data-toc-modified-id=\"SELECT-DISTICT-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>SELECT DISTICT</a></span></li><li><span><a href=\"#MAX\" data-toc-modified-id=\"MAX-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>MAX</a></span></li><li><span><a href=\"#SUMMARY\" data-toc-modified-id=\"SUMMARY-2.0.6\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>SUMMARY</a></span></li><li><span><a href=\"#STAT\" data-toc-modified-id=\"STAT-2.0.7\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>STAT</a></span></li><li><span><a href=\"#FILTER\" data-toc-modified-id=\"FILTER-2.0.8\"><span class=\"toc-item-num\">2.0.8&nbsp;&nbsp;</span>FILTER</a></span></li><li><span><a href=\"#COUNT\" data-toc-modified-id=\"COUNT-2.0.9\"><span class=\"toc-item-num\">2.0.9&nbsp;&nbsp;</span>COUNT</a></span></li><li><span><a href=\"#GROUP-BY\" data-toc-modified-id=\"GROUP-BY-2.0.10\"><span class=\"toc-item-num\">2.0.10&nbsp;&nbsp;</span>GROUP BY</a></span></li><li><span><a href=\"#FILTER-+-GROUP-BY\" data-toc-modified-id=\"FILTER-+-GROUP-BY-2.0.11\"><span class=\"toc-item-num\">2.0.11&nbsp;&nbsp;</span>FILTER + GROUP BY</a></span></li><li><span><a href=\"#SORT\" data-toc-modified-id=\"SORT-2.0.12\"><span class=\"toc-item-num\">2.0.12&nbsp;&nbsp;</span>SORT</a></span></li><li><span><a href=\"#LAMBDA\" data-toc-modified-id=\"LAMBDA-2.0.13\"><span class=\"toc-item-num\">2.0.13&nbsp;&nbsp;</span>LAMBDA</a></span></li><li><span><a href=\"#LIST-COMPREHENSION\" data-toc-modified-id=\"LIST-COMPREHENSION-2.0.14\"><span class=\"toc-item-num\">2.0.14&nbsp;&nbsp;</span>LIST COMPREHENSION</a></span></li><li><span><a href=\"#PRINT\" data-toc-modified-id=\"PRINT-2.0.15\"><span class=\"toc-item-num\">2.0.15&nbsp;&nbsp;</span>PRINT</a></span></li><li><span><a href=\"#TYPE-CONVERSION\" data-toc-modified-id=\"TYPE-CONVERSION-2.0.16\"><span class=\"toc-item-num\">2.0.16&nbsp;&nbsp;</span>TYPE CONVERSION</a></span></li><li><span><a href=\"#Batch-convert-DF-column-type\" data-toc-modified-id=\"Batch-convert-DF-column-type-2.0.17\"><span class=\"toc-item-num\">2.0.17&nbsp;&nbsp;</span>Batch convert DF column type</a></span></li><li><span><a href=\"#DROP-COLUMN\" data-toc-modified-id=\"DROP-COLUMN-2.0.18\"><span class=\"toc-item-num\">2.0.18&nbsp;&nbsp;</span>DROP COLUMN</a></span></li><li><span><a href=\"#UDF\" data-toc-modified-id=\"UDF-2.0.19\"><span class=\"toc-item-num\">2.0.19&nbsp;&nbsp;</span>UDF</a></span></li><li><span><a href=\"#SPARK-2.3-EVALUATION\" data-toc-modified-id=\"SPARK-2.3-EVALUATION-2.0.20\"><span class=\"toc-item-num\">2.0.20&nbsp;&nbsp;</span>SPARK 2.3 EVALUATION</a></span></li><li><span><a href=\"#SPARK-STATISTICAL-FUNCTIONS\" data-toc-modified-id=\"SPARK-STATISTICAL-FUNCTIONS-2.0.21\"><span class=\"toc-item-num\">2.0.21&nbsp;&nbsp;</span>SPARK STATISTICAL FUNCTIONS</a></span></li><li><span><a href=\"#SPARK-DF-DETAILS\" data-toc-modified-id=\"SPARK-DF-DETAILS-2.0.22\"><span class=\"toc-item-num\">2.0.22&nbsp;&nbsp;</span>SPARK DF DETAILS</a></span><ul class=\"toc-item\"><li><span><a href=\"#RANDOM\" data-toc-modified-id=\"RANDOM-2.0.22.1\"><span class=\"toc-item-num\">2.0.22.1&nbsp;&nbsp;</span>RANDOM</a></span></li><li><span><a href=\"#SUMMARY-&amp;-DISCRIPTIVE-STATISTICS\" data-toc-modified-id=\"SUMMARY-&amp;-DISCRIPTIVE-STATISTICS-2.0.22.2\"><span class=\"toc-item-num\">2.0.22.2&nbsp;&nbsp;</span>SUMMARY &amp; DISCRIPTIVE STATISTICS</a></span></li></ul></li><li><span><a href=\"#SPARK-CONFIGURATION\" data-toc-modified-id=\"SPARK-CONFIGURATION-2.0.23\"><span class=\"toc-item-num\">2.0.23&nbsp;&nbsp;</span>SPARK CONFIGURATION</a></span></li></ul></li></ul></li><li><span><a href=\"#GENERAL-PYTHON\" data-toc-modified-id=\"GENERAL-PYTHON-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>GENERAL PYTHON</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#3.1.2--loc-&amp;-iloc\" data-toc-modified-id=\"3.1.2--loc-&amp;-iloc-3.0.0.1\"><span class=\"toc-item-num\">3.0.0.1&nbsp;&nbsp;</span>3.1.2  loc &amp; iloc</a></span></li></ul></li><li><span><a href=\"#DICTIONARIES\" data-toc-modified-id=\"DICTIONARIES-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>DICTIONARIES</a></span></li><li><span><a href=\"#LIST-COMPREHENSION\" data-toc-modified-id=\"LIST-COMPREHENSION-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>LIST COMPREHENSION</a></span></li><li><span><a href=\"#LAMBDA-/-MAP\" data-toc-modified-id=\"LAMBDA-/-MAP-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>LAMBDA / MAP</a></span></li><li><span><a href=\"#COUNT\" data-toc-modified-id=\"COUNT-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>COUNT</a></span></li><li><span><a href=\"#TYPE-CONVERSION\" data-toc-modified-id=\"TYPE-CONVERSION-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>TYPE CONVERSION</a></span></li><li><span><a href=\"#DATAFRAME-MANIPULATIONS\" data-toc-modified-id=\"DATAFRAME-MANIPULATIONS-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;</span>DATAFRAME MANIPULATIONS</a></span></li><li><span><a href=\"#GENERAL\" data-toc-modified-id=\"GENERAL-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;</span>GENERAL</a></span></li><li><span><a href=\"#SELECT\" data-toc-modified-id=\"SELECT-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;</span>SELECT</a></span></li><li><span><a href=\"#WHERE\" data-toc-modified-id=\"WHERE-3.0.9\"><span class=\"toc-item-num\">3.0.9&nbsp;&nbsp;</span>WHERE</a></span></li><li><span><a href=\"#FILTER\" data-toc-modified-id=\"FILTER-3.0.10\"><span class=\"toc-item-num\">3.0.10&nbsp;&nbsp;</span>FILTER</a></span></li><li><span><a href=\"#STATS\" data-toc-modified-id=\"STATS-3.0.11\"><span class=\"toc-item-num\">3.0.11&nbsp;&nbsp;</span>STATS</a></span></li><li><span><a href=\"#GROUP-BY\" data-toc-modified-id=\"GROUP-BY-3.0.12\"><span class=\"toc-item-num\">3.0.12&nbsp;&nbsp;</span>GROUP BY</a></span></li><li><span><a href=\"#READ/WRITE-File/DataFrame\" data-toc-modified-id=\"READ/WRITE-File/DataFrame-3.0.13\"><span class=\"toc-item-num\">3.0.13&nbsp;&nbsp;</span>READ/WRITE File/DataFrame</a></span></li><li><span><a href=\"#MISSING-VALUES\" data-toc-modified-id=\"MISSING-VALUES-3.0.14\"><span class=\"toc-item-num\">3.0.14&nbsp;&nbsp;</span>MISSING VALUES</a></span></li><li><span><a href=\"#UNIQUE\" data-toc-modified-id=\"UNIQUE-3.0.15\"><span class=\"toc-item-num\">3.0.15&nbsp;&nbsp;</span>UNIQUE</a></span></li><li><span><a href=\"#DROP\" data-toc-modified-id=\"DROP-3.0.16\"><span class=\"toc-item-num\">3.0.16&nbsp;&nbsp;</span>DROP</a></span></li></ul></li></ul></li><li><span><a href=\"#PYTHON-DATA-SCIENCE\" data-toc-modified-id=\"PYTHON-DATA-SCIENCE-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>PYTHON DATA SCIENCE</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#GENERAL\" data-toc-modified-id=\"GENERAL-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>GENERAL</a></span></li><li><span><a href=\"#LOGISTIC-REGRESSION\" data-toc-modified-id=\"LOGISTIC-REGRESSION-4.0.0.2\"><span class=\"toc-item-num\">4.0.0.2&nbsp;&nbsp;</span>LOGISTIC REGRESSION</a></span></li><li><span><a href=\"#k-fold-Cross-validation\" data-toc-modified-id=\"k-fold-Cross-validation-4.0.0.3\"><span class=\"toc-item-num\">4.0.0.3&nbsp;&nbsp;</span>k-fold Cross validation</a></span></li><li><span><a href=\"#BASELINE-MODEL\" data-toc-modified-id=\"BASELINE-MODEL-4.0.0.4\"><span class=\"toc-item-num\">4.0.0.4&nbsp;&nbsp;</span>BASELINE MODEL</a></span></li><li><span><a href=\"#EVALUATION\" data-toc-modified-id=\"EVALUATION-4.0.0.5\"><span class=\"toc-item-num\">4.0.0.5&nbsp;&nbsp;</span>EVALUATION</a></span></li><li><span><a href=\"#FEATURE-NORMALIZARION-&amp;-STANDARDIZATION\" data-toc-modified-id=\"FEATURE-NORMALIZARION-&amp;-STANDARDIZATION-4.0.0.6\"><span class=\"toc-item-num\">4.0.0.6&nbsp;&nbsp;</span>FEATURE NORMALIZARION &amp; STANDARDIZATION</a></span></li><li><span><a href=\"#PERSISTING-A-MODEL\" data-toc-modified-id=\"PERSISTING-A-MODEL-4.0.0.7\"><span class=\"toc-item-num\">4.0.0.7&nbsp;&nbsp;</span>PERSISTING A MODEL</a></span></li></ul></li><li><span><a href=\"#PARAMS-PRINT\" data-toc-modified-id=\"PARAMS-PRINT-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>PARAMS PRINT</a></span></li></ul></li><li><span><a href=\"#1.-GBT-(GRADENT-BOOSTING-TREE)-CLASSIFIER\" data-toc-modified-id=\"1.-GBT-(GRADENT-BOOSTING-TREE)-CLASSIFIER-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>1. GBT (GRADENT BOOSTING TREE) CLASSIFIER</a></span></li></ul></li><li><span><a href=\"#PYTHON\" data-toc-modified-id=\"PYTHON-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>PYTHON</a></span><ul class=\"toc-item\"><li><span><a href=\"#LOGGING\" data-toc-modified-id=\"LOGGING-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>LOGGING</a></span><ul class=\"toc-item\"><li><span><a href=\"#PYTHON-DATA-FRAME\" data-toc-modified-id=\"PYTHON-DATA-FRAME-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>PYTHON DATA FRAME</a></span></li><li><span><a href=\"#RANGE-&amp;-RANDOM\" data-toc-modified-id=\"RANGE-&amp;-RANDOM-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>RANGE &amp; RANDOM</a></span></li><li><span><a href=\"#READ-FILE\" data-toc-modified-id=\"READ-FILE-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>READ FILE</a></span></li><li><span><a href=\"#GENERAL\" data-toc-modified-id=\"GENERAL-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>GENERAL</a></span></li></ul></li><li><span><a href=\"#JUPYTER-NOTEBOOK-RELATED\" data-toc-modified-id=\"JUPYTER-NOTEBOOK-RELATED-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>JUPYTER NOTEBOOK RELATED</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/11/solution-for-skilltest-machine-learning-revealed/\n",
    "http://www.saedsayad.com/\n",
    "Entropy & Information Gain: https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf\n",
    "https://homes.cs.washington.edu/~shapiro/EE596/notes.html\n",
    "https://courses.cs.washington.edu/courses/cse416/18sp/lectures.html\n",
    "\n",
    "https://courses.cs.washington.edu/courses/cse416/18sp/slides/\n",
    "\n",
    "https://homes.cs.washington.edu/~shapiro/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: NOT ALL CODE BLOCKS WILL RUN PROPERLY\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CODE TEMPLATE\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename='images/standadizing_data.png', height=400, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<table style=\"width:100%;height:100%\" >\n",
    "  <tr>\n",
    "    <th><img src=\"images/f1_score.png\", width=500, height=100></th>\n",
    "    <th><img src=\"images/precision_recall.png\", width=300, height=100></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE CONTENTS OF THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "file_name = os.path.join(os.path.pardir, 'src', 'data', 'test.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_name\n",
    "444 + 5555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN .PY SCRIPT FROM JUPYTER"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "get_data_processing_file = os.path.join(os.path.pardir, 'src', 'data', 'Titanic_processing_script.py')\n",
    " \n",
    "!python $get_data_processing_file\n",
    "# Output gets written below this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS TO PUSH YOUR LOCAL REPO TO GIT FOR THE FIRST TIME\n",
    "\n",
    "1) Log into https://github.com/ using your credentials\n",
    "2) Click Start a project (Repoistory name: Pluralsight_Python_data_science_Abhishek_kumar)\n",
    "3) Add/Edit/Delete code/files\n",
    "From your mac terminal window, execute below commands:\n",
    "    a) git add .\n",
    "    b) git commit -m 'logging, os info commands, read file environment variable, etc'\n",
    "    c) GANESH-PRO:notebooks ganeshpillai$ pwd\n",
    "        /Users/pinky/Downloads/LEARNING/PLURALSIGHT_Python_data_science_Abhishek_kumar/module2/titanic\n",
    "    d) GANESH-PRO:notebooks ganeshpillai$ git remote add origin https://github.com/ganesh33/Pluralsight_Python_data_science_Abhishek_kumar.git\n",
    "    e) GANESH-PRO:notebooks ganeshpillai$ git push -u origin master\n",
    "        Counting objects: 38, done.\n",
    "        Delta compression using up to 4 threads.\n",
    "        Compressing objects: 100% (35/35), done.\n",
    "        Writing objects: 100% (38/38), 1.09 MiB | 0 bytes/s, done.\n",
    "        Total 38 (delta 4), reused 0 (delta 0)\n",
    "        remote: Resolving deltas: 100% (4/4), done.\n",
    "        To https://github.com/ganesh33/Pluralsight_Python_data_science_Abhishek_kumar.git\n",
    "         * [new branch]      master -> master\n",
    "        Branch master set up to track remote branch master from origin.\n",
    "    f) GANESH-PRO:titanic ganeshpillai$ git log --oneline\n",
    "        0bb1e7e logging, os info commands, read file environment variable, etc\n",
    "        f128f7a initial commit# \n",
    "\n",
    "4) Go to below github url to view your files:\n",
    "https://github.com/ganesh33/Pluralsight_Python_data_science_Abhishek_kumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GANESH-PRO:titanic ganeshpillai$ pwd\n",
    "/Users/pinky/Downloads/LEARNING/PLURALSIGHT_Python_data_science_Abhishek_kumar/module2/titanic\n",
    "\n",
    "GANESH-PRO:titanic ganeshpillai$ git init\n",
    "Initialized empty Git repository in\n",
    "/Users/pinky/Downloads/LEARNING/PLURALSIGHT_Python_data_science_Abhishek_kumar/module2/titanic/.git/\n",
    "\n",
    "GANESH-PRO:titanic ganeshpillai$ git add .\n",
    "\n",
    "GANESH-PRO:titanic ganeshpillai$ git commit -m \"initial commit\"\n",
    "[master (root-commit) f128f7a] initial commit\n",
    " 33 files changed, 998 insertions(+)\n",
    " create mode 100644 .gitignore\n",
    " ...etc...\n",
    " create mode 100644 src/visualization/visualize.py\n",
    " create mode 100644 test_environment.py\n",
    " create mode 100644 tox.ini\n",
    "\n",
    "GANESH-PRO:titanic ganeshpillai$ git status\n",
    "On branch master\n",
    "nothing to commit, working directory clean\n",
    "\n",
    "GANESH-PRO:titanic ganeshpillai$ git log --oneline\n",
    "f128f7a initial commit\n",
    "\n",
    "GANESH-PRO:titanic ganeshpillai$ ls -a\n",
    ".\t\t\t.gitignore\t\tdata\t\t\treferences\t\tsrc\n",
    "..\t\t\tLICENSE\t\t\tdocs\t\t\treports\t\t\ttest_environment.py\n",
    ".env\t\t\tMakefile\t\tmodels\t\t\trequirements.txt\ttox.ini\n",
    ".git\t\t\tREADME.md\t\tnotebooks\t\tsetup.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUBLIC DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "https://www.data.gov/\n",
    "http://archive.ics.uci.edu/ml/datasets.html\n",
    "https://github.com/awesomedata/awesome-public-datasets\n",
    "    \n",
    "    \n",
    "https://cloud.google.com/public-datasets/\n",
    "AWS public datasets\n",
    "\n",
    "#college-scorecard\n",
    "https://catalog.data.gov/dataset/college-scorecard/resource/7b9f2bb7-21c2-4df0-9453-f332cddf61d6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.curdir\n",
    "os.pardir\n",
    "os.path.dirname(os.pardir)\n",
    "os.getcwd()\n",
    "os.path.abspath(__name__)\n",
    "os.getegid()\n",
    "os.getenv('SPARK_HOME')\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_folder= !pwd\n",
    "my_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYSPARK DATA SCIENCE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERAL:\n",
    "#adultDF.count()\n",
    "#adultDF.describe()\n",
    "#adultDF.printSchema()\n",
    "#adultDF.stat.df\n",
    "#tranformedDF.take(1)\n",
    "#tranformedDF.columns\n",
    "#all_names.tail().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT:\n",
    "adultDF.select(adultDF['EducationNum'], adultDF['Age']).show(10)\n",
    "adultDF.select('Age').show(5)\n",
    "adultDF['Age'] # O:Column<b'Age'>\n",
    "adultDF.select('Age', 'Age_int').show(10)\n",
    "\n",
    "type(adultDF.Age.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RANDOM FOREST CLASSIFICAION\").getOrCreate()\n",
    "#spark\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "adult_file='/Users/pinky/Downloads/DATA/PLURALSIGHT_Spark_machine_learning_Janani_ravi/02/adult.csv'\n",
    "\n",
    "rawData = spark.read.format('csv')\\\n",
    "                    .option('header', 'false')\\\n",
    "                    .option('ignoreLeadingWhiteSpace', 'true')\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .load(adult_file)\n",
    "adultDF = adult_data.drop('FnlWgt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT DISTICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT DISTICT\n",
    "adultDF.select('Label').distinct().show()\n",
    "adultDF.select(adultDF['Education']).distinct().show()\n",
    "adultDF.select(adultDF['WorkClass']).distinct().show()\n",
    "adultDF.select(adultDF['Education'], adultDF['EducationNum']).distinct().sort(adultDF['EducationNum']).show()\n",
    "adultDF.select('Age').filter(adultDF['Age'] > 85).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAX \n",
    "adultDF.select(max('Age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY\n",
    "adultDF.select('Age', 'CapitalGain', 'HoursPerWeek').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAT:\n",
    "adultDF.stat.freqItems(['Age']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER\n",
    "adultDF.filter('Gender=\"Male\"').show(5)\n",
    "adultDF.filter('Age =50').show(3)\n",
    "adultDF.filter('Gender=\"Male\"').select('Age', 'CapitalGain', 'HoursPerWeek').summary().show()\n",
    "adultDF.filter('Age' == max('Age')).show()\n",
    "adultDF.filter('Age == 90').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  trainingData2.select('Descript').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP BY:\n",
    "adultDF.groupBy('NativeCountry').count().sort('count', ascending=False).show(10)\n",
    "adultDF.select('EducationNum','Age').groupby(adultDF['EducationNum']).count().show()\n",
    "adultDF.select('Gender').groupby('Gender').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER + GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER + GROUP BY:\n",
    "adultDF.filter('Age == 90').groupBy('NativeCountry').count().show()\n",
    "adultDF.filter(adultDF['WorkClass'] == 'Without-pay').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SORT\n",
    "df.sort(\"age\", ascending=False).collect()\n",
    "naiveBayesPredictions.orderBy('probability').sort('probability', ascenging=False).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAMBDA\n",
    "lambda x: x ** 2, list6\n",
    "list(map(lambda x: x ** 2, list6))\n",
    "list(map(lambda x: x **2, (filter(lambda x: x %2 == 0, range(1,20)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIST COMPREHENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST COMPREHENSION\n",
    "list(x ** 3 for x in list5) # list comprehension\n",
    "list(x % 2 == 0 for x in list5) # filter using list comprehension\n",
    "list(x for x in list5 if x % 2 == 0) # filter & map using list comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT A LIST\n",
    "print(*my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYPE CONVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType \n",
    "adultDF = adultDF.withColumn('Age_int', adultDF['Age'].cast(FloatType()))\n",
    "\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "changedTypedf = joindf.withColumn(\"label\", joindf[\"show\"].cast(DoubleType()))\n",
    "#or short string:\n",
    "changedTypedf = joindf.withColumn(\"label\", joindf[\"show\"].cast(\"double\"))\n",
    "\n",
    "#REPLACE EXISTING COLUMN\n",
    "changedTypedf = joindf.withColumn(\"show\", joindf[\"show\"].cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch convert DF column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String Index all columns\n",
    "indexers = [StringIndexer(inputCol=i, outputCol=i + '_indexed', handleInvalid='keep') for i in categoricalFeatures]\n",
    "for indexer in indexers:\n",
    "    print(indexer.getOutputCol())\n",
    "    \n",
    "#Covert to Double all columns\n",
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "for col_name in columns_list:\n",
    "    htrainDF =  htrainDF.withColumn(col_name + '_double', htrainDF[col_name].cast(DoubleType()))\n",
    "    htrainDF =  htrainDF.drop(col_name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htrainDF = htrainDF.drop('crim_float')\n",
    "\n",
    "drop_list = ['a column', 'another column', ...]\n",
    "df.select([column for column in df.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "maturity_udf = udf(lambda age: \"adult\" if age >=18 else \"child\") #, StringType())\n",
    "#maturity_udf = udf(lambda age: \"adult\" if age >=18 else \"child\") , StringType()) #same as above\n",
    " \n",
    "#df = spark.createDataFrame([('Alice', 1)], ['name', 'age'])\n",
    "df = spark.createDataFrame([('Alice', 1), ('Alice2', 50)], ['name', 'age'])\n",
    "df.show()\n",
    "df_new= df.withColumn(\"maturity\", maturity_udf(df.age))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK 2.3 EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy\n",
    "weightedPrecision\n",
    "weightedRecall\n",
    "f1\n",
    "#MulticlassClassificationEvaluator evaluator4 = new MulticlassClassificationEvaluator().setMetricName(\"f1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK STATISTICAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK DF DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htrainDF.describe().toPandas()\n",
    "htrainDF.describe().toPandas().T\n",
    "\n",
    "htrainDF.describe().show()\n",
    "htrainDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM\n",
    "from pyspark.sql.functions import rand, randn\n",
    "df1 = spark.range(1,10)\n",
    "df1.toPandas().T\n",
    "\n",
    "column1 = rand(seed=101)  #uniform distribution\n",
    "\n",
    "column2 = randn(seed=202).alias('normal') # normal distribution\n",
    "df2 = df1.withColumn('uniform', column1).withColumn('normal2', column2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY & DISCRIPTIVE STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUMMARY & DISCRIPTIVE STATISTICS\n",
    "\n",
    "#mean, min, max, kurtosis, length\n",
    "from pyspark.sql.functions import mean, min, max, stddev, variance, avg, kurtosis, length\n",
    "df2.select(mean('normal2'), min('id'), max('uniform'), stddev('normal2'), variance('id'), avg('normal2')).show()\n",
    "df2.select(kurtosis('normal2')).show()\n",
    "df2.select(length('id')).show()\n",
    "\n",
    "#corr & covar\n",
    "df3 = spark.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "df4 = df3.withColumn('id2', df3['id'] * 2)\n",
    "\n",
    "df4.stat.corr('rand1', 'rand2')\n",
    "df4.stat.cov('id', 'id2')\n",
    "\n",
    "#CROSS TABULATION\n",
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df6 = spark.createDataFrame([(names[i % 3], items[i % 5 ]) for i in range(10)], ['name', 'item'])\n",
    "df6.stat.crosstab('name', 'item').show()\n",
    "\n",
    "#FREQUENT ITEMS\n",
    "df6.stat.freqItems(['name', 'item'], 0.5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers (TO MAKE .toPandas() work)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GENERAL PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name.split(',')[1] \n",
    "title.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2  loc & iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc: (String based)\n",
    "    --> is label based index as string\n",
    "    --> takes slices based on labels\n",
    "    --> includes last element\n",
    "iloc: (Integer based)\n",
    "    --> bases on index's position\n",
    "    --> uses obervasion's position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc gets rows (or columns) with particular labels from the index.\n",
    "iloc gets rows (or columns) at particular positions in the index (so it only takes integers).\n",
    "ix usually tries to behave like loc but falls back to behaving like iloc if a label is not present in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Integer Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw_data_path = os.path.join(os.path.pardir, 'data', 'raw') #'../data/raw\n",
    "\n",
    "train_data_path = os.path.join(raw_data_path, 'train.csv') #'../data/raw/train.csv'\n",
    "df1 = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[['PassengerId', 'Pclass', 'Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()\n",
    "df2.loc[1:3]\n",
    "df2.iloc[1:3]\n",
    "df2.loc[2]\n",
    "df2.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### String Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3.index = df3.Name\n",
    "df3.head()\n",
    "#df3.loc[1:3] #TypeError: cannot do slice indexing\n",
    "df3.iloc[1:3]\n",
    "#df3.loc[2] #TypeError: cannot do label indexing\n",
    "df3.loc['Heikkinen, Miss. Laina']\n",
    "df3.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2.copy()\n",
    "df4.set_index(\"Name\", inplace=True)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df2.copy()\n",
    "df5.set_index(\"Name\", inplace=False)\n",
    "df5.head()\n",
    "df5.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICTIONARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dictionary.keys()\n",
    "title_dictionary.values()\n",
    "title_dictionary.get('dona')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIST COMPREHENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAMBDA / MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF.Name.map(lambda x: getTitle(x)).head()\n",
    "combinedDF.Name.map(lambda x: getTitle(x)).unique()\n",
    "combinedDF.Name.map(lambda x: getTitle(x)).unique()\n",
    "combinedDF.Name.map(lambda x: getTitle(x)).value_counts(ascending=False).head()\n",
    "combinedDF.Name.map(lambda x: getTitle(x)).unique().tolist()\n",
    "combinedDF['Title'] = combinedDF.Name.map(lambda x: getStandardTitle(getTitle(x)))\n",
    "combinedDF['Deck'] = combinedDF.Cabin.map(lambda x: getDeck(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "len(male_passengers)\n",
    "\n",
    "# Value count\n",
    "combinedDF.Embarked.value_counts()\n",
    "df5.Age.value_counts(dropna=False)\n",
    "combinedDF.Age.value_counts(dropna=False).head()\n",
    "\n",
    "# Size \n",
    "combinedDF.groupby('Embarked').size() \n",
    "\n",
    "# Count as list\n",
    "trainDF.count().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYPE CONVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainDF.loc[:, 'Adult_No':].values.astype('float')\n",
    "\n",
    "drinks = pd.read_csv(url, dtype={'beer_servings':float}) # read as float\n",
    "autoDF['price_numeric'] = pd.to_numeric(autoDF.price, errors='coerce') # ignore errors  # float64\n",
    "autoDF['price_test'] = autoDF.price.astype(float) # float64\n",
    "autoDF['price_test4'] = autoDF.price.astype(int) # int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME MANIPULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF.head()\n",
    "type(combinedDF.Name)\n",
    "combinedDF.index\n",
    "combinedDF.columns\n",
    "combinedDF.Embarked.value_counts().to_frame()\n",
    "\n",
    "df5 = combinedDF.copy()\n",
    "trainDF.keys()\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF['Survived'] = -888\n",
    "combinedDF.Name[0:5]  #select first 5 from series\n",
    "combinedDF[['Name', 'Age']][6:8]  # select multiple columns\n",
    "trainDF.loc[800:804] # select based on index values(rows)\n",
    "trainDF.iloc[800:804]  #select based on index values(rows) & columns\n",
    "trainDF.loc[2:5, ['Age', 'Pclass']]  #select based on index values(rows) & columns\n",
    "trainDF.iloc[2:5,3:8] # use iloc for postion based indexing\n",
    "trainDF.iloc[2:5,3:8] # use iloc for postion based indexing\n",
    "combinedDF.groupby(['Survived', 'Embarked']).Embarked.count().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF = pd.concat((trainDF, testDF), sort=True)  #acis=0 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF['Adult'] = np.where(combinedDF.Age >= 18, 'Yes', 'No')\n",
    "combinedDF['IsMale'] = np.where(combinedDF.Sex == 'male', 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF.loc[combinedDF.Sex == 'male']\n",
    "combinedDF.loc[(combinedDF.Sex == 'male') & (combinedDF.Pclass == 1)]\n",
    "combinedDF[combinedDF.Survived != -888].Survived.value_counts()\n",
    "\n",
    "combinedDF[combinedDF.Embarked.isnull()] # BEST RESULT for displaying obervations with nan's\n",
    "\n",
    "combinedDF.Age.value_counts(dropna=False, ascending=False).head()\n",
    "\n",
    "combinedDF[(combinedDF['Embarked'] == 'S') & (combinedDF['Pclass'] == 3)].Fare.value_counts(ascending=False).head()\n",
    "\n",
    "combinedDF.loc[combinedDF.Fare == combinedDF.Fare.max()]\n",
    "combinedDF.loc[combinedDF.Cabin =='T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.info()\n",
    "combinedDF.describe()\n",
    "combinedDF.describe(include='all')\n",
    "\n",
    "combinedDF.Sex.value_counts()\n",
    "combinedDF.Age.mean()\n",
    "combinedDF.Age.median()\n",
    "combinedDF.Fare.min(), combinedDF.Fare.max()\n",
    "combinedDF.Fare.quantile(0.25), combinedDF.Fare.quantile(0.5), combinedDF.Fare.quantile(0.75)\n",
    "combinedDF.Fare.var(), combinedDF.Fare.std() \n",
    "\n",
    "combinedDF.Embarked.mode()\n",
    "combinedDF.groupby(['Pclass', 'Embarked']).Fare.median()\n",
    "\n",
    "pd.crosstab(combinedDF.Sex, combinedDF.Pclass)\n",
    "combinedDF.pivot_table(index='Sex', columns='Pclass', values='Age', aggfunc='mean')\n",
    "combinedDF.groupby(['Sex', 'Pclass']).Age.mean()\n",
    "combinedDF.groupby(['Sex', 'Pclass']).Age.mean().unstack()\n",
    "\n",
    "combinedDF.loc[(combinedDF.Embarked =='S') & (combinedDF.Pclass == 3), 'Fare'].median()\n",
    "\n",
    "np.log(combinedDF.Fare + 1.0)\n",
    "\n",
    "\n",
    "np.mean(y_train), np.mean(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF.groupby(['Pclass','Sex'])['Fare', 'Age'].median() # display selected columns\n",
    "combinedDF.groupby(['Pclass']).agg({'Fare':'mean', 'Age':'median'})  # using agg\n",
    "\n",
    "\n",
    "aggregations = {\n",
    "    'Fare': {\n",
    "            'mean_fare' : 'mean' ,\n",
    "            'max_fare' : max,\n",
    "            'min_fare' : np.min\n",
    "            },\n",
    "    'Age': {\n",
    "        'range_age' : lambda x: max(x) - min(x)\n",
    "    }\n",
    "}\n",
    "combinedDF.groupby('Pclass').agg(aggregations)\n",
    "\n",
    "combinedDF.groupby('Sex').Age.transform('median').head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ/WRITE File/DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "trainDF = pd.read_csv(train_data_path, index_col='PassengerId')\n",
    "\n",
    "combinedDF.loc[combinedDF.Survived != -888].to_csv(write_train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: \n",
    "#       -> Deletion \n",
    "#       -> Imputation\n",
    "#          --> Mean imputation  (average) (result IS impacted by outliers)\n",
    "#          --> Median imputation (middle value) (result NOT impacted by outliers)\n",
    "#          --> Mode imputation (most occurring) (for categorical features)\n",
    "#          --> Forward fill (replace with previous value)\n",
    "#          --> Backward fill (replace with next value)\n",
    "#          --> Predictive Model (predict using models)\n",
    "\n",
    "\n",
    "combinedDF.Embarked.nunique(dropna=False)\n",
    "combinedDF.Embarked.fillna('C', inplace=True)\n",
    "combinedDF[combinedDF.Age.isna()]\n",
    "df5.Age.fillna(combinedDF.groupby('Sex').Age.median(), inplace=True)\n",
    "df5.Age.fillna(combinedDF.groupby('Sex').Age.transform('median'), inplace=True)\n",
    "combinedDF.Age.fillna(combinedDF.groupby('Title').Age.transform('median'), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF.Embarked.unique()\n",
    "combinedDF.Cabin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF.drop(['Cabin','Name','Ticket','Parch','SibSp','Sex'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTHON DATA SCIENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDF = pd.get_dummies(combinedDF, columns=['Deck', 'Pclass', 'Title', 'Embarked', 'Adult'])\n",
    "\n",
    "y = trainDF.Survived.ravel()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrModel = LogisticRegression(random_state=0)\n",
    "lrModel.fit(X_train, y_train)\n",
    "lr_prediction = lrModel.predict(X_test)\n",
    "\n",
    "lrModel.get_params()\n",
    "lrModel.densify\n",
    "lrModel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lrGridSearch = GridSearchCV(lrModel, param_grid=parameters, cv=3)\n",
    "lrGridSearch.fit(X_train, y_train)\n",
    "lrGridSearch.best_estimator_\n",
    "\n",
    "lrGridSearch.best_params_\n",
    "lrGridSearch.best_score_\n",
    "bestModel = lrGridSearch.best_estimator_\n",
    "best_predictions = bestModel.predict(X_test)\n",
    "\n",
    "\n",
    "# EXAMPLE2\n",
    "parameters = {'C':[1.0, 10.0, 50.0, 100.0, 1000.0]}\n",
    "lrGridSearch = GridSearchCV(lrModel, param_grid=parameters, cv=3)\n",
    "lrGridSearch.fit(X_train_scaled, y_train)\n",
    "bestModel = lrGridSearch.best_estimator_\n",
    "best_predictions = bestModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASELINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train), np.mean(y_test)\n",
    "\n",
    "# OPTIONS: \"most_frequent\", \"stratified\", \"uniform\",  \"constant\", \"prior\"\n",
    "dummyModel = DummyClassifier(random_state=0, strategy='most_frequent')\n",
    "dummyModel.fit(X=X_train, y = y_train)\n",
    "prediction = dummyModel.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "accuracy_score(y_test, prediction)\n",
    "precision_score(y_test, prediction)\n",
    "recall_score(y_test, prediction)\n",
    "confusion_matrix(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE NORMALIZARION & STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "minMaxScaler = MinMaxScaler()\n",
    "X_train_scaled = minMaxScaler.fit_transform(X=X_train)\n",
    "X_train_scaled.min(), X_train_scaled.max()\n",
    "X_test_scaled = minMaxScaler.fit_transform(X=X_test)\n",
    "\n",
    "#STANDARIZATION\n",
    "stdScaler = StandardScaler()\n",
    "X_train_scaled2 = stdScaler.fit_transform(X_train_scaled)\n",
    "X_train_scaled2.min(), X_train_scaled2.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PERSISTING A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE\n",
    "mport pickle\n",
    "lr_model_path = os.path.join(os.path.pardir, 'models', 'lrModel.pkl') #'../models/lrModel.pkl'\n",
    "model_pickle = open(lr_model_path, 'wb')\n",
    "pickle.dump(lrModel, model_pickle)\n",
    "model_pickle.close()\n",
    "\n",
    "# READ BACK\n",
    "model_pickle_r = open(lr_model_path, 'rb')\n",
    "lrModelLoaded = pickle.load(model_pickle_r)\n",
    "model_pickle_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "#PYTHON DATA SCIENCE\n",
    "adultDF = adultDF.replace('?', None)\n",
    "adultDF.dropna(how='any')\n",
    "\n",
    "\n",
    "adultDF = adultDF.replace('?', None)\n",
    "adultDF.dropna(how='any')\n",
    "\n",
    "adultDF.filter(adultDF['WorkClass'] == 'Without-pay').show(3)\n",
    "'''\n",
    "String values --> numerical values:       <column>.cast(FloatType())\n",
    "Categorical Variables --> numeric         String Indexer\n",
    "Indexed columns --> Vector                OneHotEncoderEstimator\n",
    "Create feature vector(of input columns)   VectorAssembler\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import min, max, mean\n",
    "\n",
    "#STEP1: Convert string values into numeric values (not needed for my dataset)\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col\n",
    "adultDF = adultDF.withColumn('Age_int', adultDF['Age'].cast(FloatType()))\n",
    "\n",
    "#STEP2: Convert Categorical Variables into numeric\n",
    "#WorkClass --> WorkClass_index\n",
    "#'State-gov' -->  3.0\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol='WorkClass', outputCol='WorkClass_index')\n",
    "indexedDF = stringIndexer.fit(adultDF).transform(adultDF)\n",
    "#indexedDF.select('WorkClass', 'WorkClass_index').distinct().show(10)\n",
    "\n",
    "#STEP3: Convert Indexed Categorical Variables to vectors\n",
    "#WorkClassindex -->WorkClass_encoded\n",
    "#2.0 -->   (6,[2],[1.0])\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "oneHotEncoderEstimator = OneHotEncoderEstimator(inputCols=['WorkClass_index'], outputCols=['WorkClass_encoded'])\n",
    "ncodedDF = oneHotEncoderEstimator.fit(indexedDF).transform(indexedDF)\n",
    "encodedDF.select('WorkClass', 'WorkClass_index', 'WorkClass_encoded').distinct().show(10)\n",
    "\n",
    "encodedDF = encodedDF.withColumnRenamed('WorkClass_index','WorkClass_index_one_off')\\\n",
    "                     .withColumnRenamed('WorkClass_encoded','WorkClass_encoded_one_off')\n",
    "\n",
    "#STEP4: String Indexing all other categorical features\n",
    "categoricalFeatures = [  'WorkClass',\n",
    "                         'Education',\n",
    "                         'MaritalStatus',\n",
    "                         'Occupation',\n",
    "                         'Relationship',\n",
    "                         'Race',\n",
    "                         'Gender',\n",
    "                         'NativeCountry']\n",
    "indexers = [StringIndexer(inputCol=i, outputCol=i + '_indexed', handleInvalid='keep') for i in categoricalFeatures]\n",
    "#for indexer in indexers:\n",
    "  #print(indexer.getOutputCol())\n",
    "\n",
    "encoders = [OneHotEncoderEstimator(inputCols=[i + '_indexed'], outputCols=[i + '_encoded']) for i in categoricalFeatures]\n",
    "#for encoder in encoders:\n",
    "   #print(encoder.getInputCols(), '\\t', encoder.getOutputCols())\n",
    "\n",
    "#STEP5: Convert label column into index\n",
    "labelIndexer = StringIndexer(inputCol='Label', outputCol='Label_indexed')\n",
    "\n",
    "#STEP6: USE PIPELINE\n",
    "##import pyspark.ml.Pipeline\n",
    "from pyspark.ml import Pipeline \n",
    "pipeline = Pipeline(stages=indexers + encoders + [labelIndexer])\n",
    "tranformedDF = pipeline.fit(encodedDF).transform(encodedDF)\n",
    "#tranformedDF.take(1)\n",
    "#tranformedDF.select('Label', 'Label_indexed').distinct().show()\n",
    "#tranformedDF.columns\n",
    "    \n",
    "#STEP7: Vectorize all feature columns together\n",
    "requiredFeatures = ['Age',\n",
    " 'EducationNum',\n",
    " 'CapitalGain',\n",
    " 'CapitalLoss',\n",
    " 'HoursPerWeek',\n",
    " 'WorkClass_encoded',\n",
    " 'Education_encoded',\n",
    " 'MaritalStatus_encoded',\n",
    " 'Occupation_encoded',\n",
    " 'Relationship_encoded',\n",
    " 'Race_encoded',\n",
    " 'Gender_encoded',\n",
    " 'NativeCountry_encoded']\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=requiredFeatures, outputCol='features')\n",
    "vectorizedDF = assembler.transform(tranformedDF)\n",
    "#vectorizedDF.select('features').show(5, truncate=False)\n",
    "#vectorizedDF.select('WorkClass_index_one_off', 'WorkClass_encoded_one_off', \n",
    "           #'WorkClass_indexed','WorkClass_encoded').show(5, truncate=False)\n",
    "    \n",
    "#STEP8: Split the data\n",
    "trainingDataDF, testDataDF = vectorizedDF.randomSplit([0.8,0.2])\n",
    "#trainingDataDF.count(), testDataDF.count()\n",
    "\n",
    "#STEP9: Create RANDOMFOREST Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "randomForestClassifier = RandomForestClassifier(maxDepth=5, \n",
    "                                                labelCol='Label_indexed', \n",
    "                                                featuresCol= 'features')\n",
    "\n",
    "#NOTE: BELOW STEP DOESN'T WORK BEACUSE ALL THE stages ARE ALREADY APPLIED. \n",
    "#EITHER DROP ALL THSOSE NEW COLUMNS OR \n",
    "#RECREATE THE ORIGINAL DATASET OR \n",
    "#APPLY ONLY THE classification STEPstep\n",
    "'''\n",
    "pipelineAll  = Pipeline(stages = indexers + \n",
    "                                 encoders + \n",
    "                                [labelIndexer] + \n",
    "                                [assembler] + \n",
    "                                [randomForestClassifier]) \n",
    "model = pipelineAll.fit(trainingDataDF)\n",
    "'''\n",
    "model = randomForestClassifier.fit(trainingDataDF)\n",
    "\n",
    "#{param[0].name: param[1] for param in model.extractParamMap().items()}\n",
    "#model.explainParam('maxDepth')\n",
    "\n",
    "#dict_values = model.extractParamMap().values()\n",
    "#list(dict_values)\n",
    "#list(model.extractParamMap().values())[7]\n",
    "#model.getNumTrees\n",
    "#model.featuresCol\n",
    "#model.impurity  # SAME AS model.getParam('impurity')\n",
    "#model.numClasses, model.numFeatures\n",
    "#model.labelCol, model.featuresCol\n",
    "\n",
    "#STEP10: PREDICT ON TEST DATA\n",
    "predictedDF = model.transform(testDataDF)\n",
    "#predictedDF.select('Label_indexed', 'prediction').show(10)\n",
    "#PLOT\n",
    "#predictedDF.select('Label_indexed', 'prediction').toPandas().plot(kind='scatter', x='Label_indexed', y='prediction')\n",
    "\n",
    "#STEP11 Evaluate the model(evaluate the predictions)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "binaryClassificationEvaluator = BinaryClassificationEvaluator(\n",
    "                                        labelCol='Label_indexed', \n",
    "                                        rawPredictionCol='prediction', \n",
    "                                        metricName='areaUnderROC')\n",
    "accuracy = binaryClassificationEvaluator.evaluate(predictedOnlyDF)\n",
    "#accuracy\n",
    "#predictedOnlyPandasDF = predictedOnlyDF.toPandas()\n",
    "#predictedOnlyPandasDF.loc[predictedOnlyPandasDF['Label_indexed'] \n",
    "                      #  != predictedOnlyPandasDF['prediction']][0:5]\n",
    "\n",
    "\n",
    "###STEP12: Try again with different depth\n",
    "randomForestClassifier10 = RandomForestClassifier(maxDepth=10, \n",
    "                                                labelCol='Label_indexed', \n",
    "                                                featuresCol= 'features')\n",
    "model10 = randomForestClassifier10.fit(trainingDataDF)\n",
    "preditions10 = model10.transform(testDataDF)\n",
    "accuracy10 = binaryClassificationEvaluator.evaluate(preditions10)\n",
    "#accuracy10\n",
    "    \n",
    "maxBins   = range(10, 33, 11)  # max value = 32   \n",
    "maxBins   = range(10, 33, 11)  # max value = 32    \n",
    "my_parameter_list = list(zip(maxDepths,maxBins)) #[(10, 10), (20, 21), (30, 32)]    \n",
    "#for x in my_parameter_list\n",
    "  #print(x[0],x[1])   \n",
    "\n",
    "randomForestClassifierList = []\n",
    "\n",
    "randomForestClassifierList.append(\n",
    "                            RandomForestClassifier(maxDepth=x[0], maxBins=x[1],\n",
    "                                                   labelCol='Label_indexed', \n",
    "                                                   featuresCol= 'features'))\n",
    "    \n",
    "#len(randomForestClassifierList\n",
    "    \n",
    "def classify(classifier):\n",
    "    model = classifier.fit(trainingDataDF)\n",
    "    prediction = model.transform(testDataDF)\n",
    "    accuracy = binaryClassificationEvaluator.evaluate(prediction) \n",
    "    print('MAXDEPTH:', list(model.extractParamMap().values())[7], \n",
    "          'MAXBINS:', list(model.extractParamMap().values())[6],\n",
    "          'ACCURACY:', accuracy)    \n",
    "    \n",
    "# USING LIST COMPREHENSION\n",
    "[classify(r) for r in randomForestClassifierList]    \n",
    "#O:\n",
    "#MAXDEPTH: 10 MAXBINS: 10 ACCURACY: 0.7187703161570407\n",
    "#MAXDEPTH: 20 MAXBINS: 21 ACCURACY: 0.7622451075477894\n",
    "#MAXDEPTH: 30 MAXBINS: 32 ACCURACY: 0.7754354575437111\n",
    "\n",
    "#STEP13: \n",
    "'''\n",
    "Two Types of Hyper Parameter Tuning (aka Model Selection) \n",
    "    1) Cross Validator\n",
    "    2) TrainValidationSplit\n",
    "'''\n",
    "#CROSS VALIDATOR\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid =   ParamGridBuilder().addGrid(randomForestClassifier10.maxDepth, [4,6]) \\\n",
    "                                .addGrid(randomForestClassifier10.maxBins, [20,60]) \\\n",
    "                                .addGrid(randomForestClassifier10.numTrees, [5,20]) \\\n",
    "                                .build()\n",
    "\n",
    "crossValidator = CrossValidator(estimator=randomForestClassifier10, \n",
    "                                evaluator=binaryClassificationEvaluator,\n",
    "                                estimatorParamMaps=paramGrid,\n",
    "                                numFolds=5)\n",
    "\n",
    "\n",
    "\n",
    "STEP14: Fit training data to all the models\n",
    "cvModel = crossValidator.fit(trainingDataDF) # THIS STEP WILL TAKE A LONG TIME\n",
    "#cvModel.avgMetrics   # (2 x 2 x 2 iterations)\n",
    "\n",
    "#STEP15: GET THE BEST MODEL\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "list(bestModel.extractParamMap().values())\n",
    "{param[0].name: param[1] for param in bestModel.extractParamMap().items()}\n",
    "\n",
    "#STEP16: Predict using the test data\n",
    "bestModelPredictions = bestModel.transform(testDataDF)\n",
    "bestModelAccuracy = binaryClassificationEvaluator.evaluate(bestModelPredictions)\n",
    "#bestModelAccuracy #0.7170562491880605\n",
    "\n",
    "#STEP17: Save the model for future use\n",
    "bestModel.write().overwrite().save(\"random_forest_CrossValidator_best_model\")\n",
    "\n",
    "#STEP 17.1: READ BACK\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "saved_model = RandomForestClassificationModel.load(\"random_forest_CrossValidator_best_model\")\n",
    "_, testDataDF2 = vectorizedDF.randomSplit([0.9,0.1])\n",
    "saved_model_predictions = saved_model.transform(testDataDF2)\n",
    "saved_model_accuracy = binaryClassificationEvaluator.evaluate(saved_model_predictions)\n",
    "#saved_model_accuracy\n",
    "\n",
    "#STEP18: Hyper Parameter Tuning (aka Model Selection) (TrainValidationSplit)\n",
    "#Evaluates each combination of parameters once\n",
    "from pyspark.ml.tuning import TrainValidationSplit    \n",
    "paramGrid2 = ParamGridBuilder().addGrid(randomForestClassifier10.maxDepth, [4,6])\n",
    "                               .addGrid(randomForestClassifier10.numTrees, [5,20])\n",
    "                               .build()\n",
    "trainingData90, testData10 = vectorizedDF.randomSplit([0.9, 0.1], seed=12345)\n",
    "tvs = TrainValidationSplit(estimator=bestModel, estimatorParamMaps=paramGrid2,\n",
    "                               evaluator=BinaryClassificationEvaluator, trainRatio=0.9)\n",
    "\n",
    "tvsModel = tvs.fit(trainingData90)\n",
    "#ERROR: AttributeError: 'RandomForestClassificationModel' object has no attribute 'fitMultiple'\n",
    "#TO DO: FIX THIS LATER  \n",
    "\n",
    "#STEP19: EVALUATION using BinaryClassificationMetrics\n",
    "predictedOnlyDF2 = predictedOnlyDF.withColumnRenamed('Label_indexed', 'label')\n",
    "#predictedOnlyDF2.columns  O:['label', 'prediction']\n",
    "predictionAndLabels = predictedOnlyDF2.rdd.map(lambda row: (float(row.label), float(row.prediction)))\n",
    "binaryClassificationMetrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "binaryClassificationMetrics.areaUnderPR #0.4040690039202276\n",
    "binaryClassificationMetrics.areaUnderROC #0.8158521147102044\n",
    "\n",
    "#STEP20: TO DO: EVALUATION using MulticlassMetrics\n",
    "#TO DO: Select the best model and do the evaluation in it( same as for CrossValidator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMS PRINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in bestModel.extractParamMap().items():\n",
    "    print (key.name, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GBT (GRADENT BOOSTING TREE) CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GBT (GRADENT BOOSTING TREE) CLASSIFIER\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbtClassifier = GBTClassifier(labelCol='Label_indexed',  featuresCol = 'features',  maxIter=30)\n",
    "gbtModel = gbtClassifier.fit(trainingDataDF)\n",
    "gbtPredictions = gbtModel.transform(testDataDF)\n",
    "gbtAccuracy = binaryClassificationEvaluator.evaluate(gbtPredictions)\n",
    "#gbtAccuracy #0.7647927357765553\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Levels: DEBUG, INFO, WARNING, ERROR, CRITICAL or NOTSET\n",
    "# NOTE: NOTSET means all messages will be logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "'''\n",
    "# create file handler which logs even debug messages\n",
    "fileLogger = logging.FileHandler('pluralsight_python_datascience_abhishek.log')\n",
    "fileLogger.setLevel(logging.DEBUG) \n",
    "\n",
    "# create console handler with a higher log level\n",
    "consoleStreamingLogger = logging.StreamHandler()\n",
    "consoleStreamingLogger.setLevel(logging.ERROR)\n",
    " \n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "consoleStreamingLogger.setFormatter(formatter)\n",
    "consoleStreamingLogger.setFormatter(formatter)\n",
    "\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fileLogger)\n",
    "logger.addHandler(consoleStreamingLogger)\n",
    "\n",
    "logger = logging.getLogger(name=__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "print(logger.level)\n",
    "logger.level = logging.DEBUG\n",
    "\n",
    "logger.debug('This is debug message')\n",
    "logger.info('This is info message')\n",
    "# logger.warn('This is warn message') # DeprecationWarning: The 'warn' method is deprecated\n",
    "logger.warning('This is warning message')\n",
    "logger.error('This is error message')  # 2018-09-11 19:29:27,348 - __main__ - ERROR - This is error message\n",
    "logger.fatal('This is fatal message') # 2018-09-11 19:29:27,362 - __main__ - CRITICAL - This is fatal message\n",
    "logger.critical('This is critical message') # 2018-09-11 19:29:27,381 - __main__ - CRITICAL - This is critical message\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# WORKS\n",
    "import logging\n",
    "from logging.config import fileConfig\n",
    "\n",
    "#logging.config.fileConfig('titanic/logging/logging.conf')\n",
    "fileConfig('titanic/logging/logging.conf')\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 'application' code\n",
    "logger.debug('debug message')\n",
    "logger.info('info message')\n",
    "logger.warning('warn message')\n",
    "logger.error('error message')\n",
    "logger.critical('critical message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### TO DO: LOGGING USING YAML config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### PYTHON DATA FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PYTHON DATA FRAME\n",
    "pd.DataFrame(data = [list(ucase), list(lcase)], index=(\"upper\", \"lower\"), columns = list(ucase))\n",
    "pd.DataFrame(np.random.randint(1,100,12).reshape(3,4))\n",
    "forecast = pd.DataFrame({\"high\": high_temps, \"low\": low_temps}, index=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"])\n",
    "\n",
    "df1.head()\n",
    "df1.index= np.arange(1,27)\n",
    "\n",
    "df1.dtypes  # column types\n",
    "df1.index = df1['lower'] # set index\n",
    "df1.sort_values('numbers').head() # sort\n",
    "df1['numbers'].head() # select column\n",
    "df1[['numbers', 'lower']].head(3) # select columns\n",
    "df1.iloc[20] # select row\n",
    "df1.iloc[15:18] # select rows\n",
    "\n",
    "df1.describe() # stats\n",
    "\n",
    "df1['numbers'] = np.random.randint(1,100,26) # adding new column\n",
    "forecast['difference']= forecast['high'] - forecast['low'] # adding new column\n",
    "df['three']=pd.Series([10,20,30],index=['a','b','c']) # add series as a new column\n",
    "\n",
    "df2 = pd.read_csv(s2, parse_dates=['Date'])   # Date format gets changed\n",
    "df2.set_index('Date', inplace=True) # df2 gets changed\n",
    "\n",
    "df2.plot(kind='area')\n",
    "\n",
    "\n",
    "del df['one']  # delete a column \n",
    "df.pop('two')  # delete a column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### RANGE & RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#RANGE & RANDOM\n",
    "A1 = range(1,10) #O: 1 2 3 4 5 6 7 8 9\n",
    "A = np.arange(10) #O: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "B = np.random.randn(10)\n",
    "#O: Array([-1.49553106, -0.84957435,  0.63583913, ...,  1.17658258,  2.43626867, -1.50656064])\n",
    "colors = np.random.randint(0,10,10) #array([0, 9, 8, 8, 8, 5, 7, 2, 2, 3])\n",
    "D = np.random.random(10)\n",
    "#O: array([ 0.55010528,  0.57947712,  0.23147827, ..., 0.01487446,  0.12264088,  0.34392944])\n",
    "E = np.random.random_sample(10)\n",
    "#O: array([ 0.55010528,  0.57947712,  0.23147827, ..., 0.01487446,  0.12264088,  0.34392944])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### READ FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pandasDF = pd.read_csv(airpassengers_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pandasDF.dtypes\n",
    "type(pandasDF)\n",
    "pandasDF.index\n",
    "pandasDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## JUPYTER NOTEBOOK RELATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script Pluralsight_Python_data_science_Abhishek_kumar_2.ipynb"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "727px",
    "left": "37px",
    "top": "134px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

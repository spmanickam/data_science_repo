{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.U_CV_JPortilla_12(Object_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECT TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nObject Tracking techniques:\\n    -> Optical Flow\\n    -> Meanshift and CamShift\\n    -> OpenCV built-in Tracking APIs\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Object Tracking techniques:\n",
    "    -> Optical Flow\n",
    "    -> Meanshift and CamShift\n",
    "    -> OpenCV built-in Tracking APIs\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n-> Optical flow is the pattern of apparent motion if image objects between two consecutive frames caused by the movement of object or camera\\n-> (Our) Optical Flow analysis here have few assumptions:\\n    --> The pixel intentsities of an object do not change between consecutive feames(Ex you're not trying to track a blinking bulb)\\n    --> Neighbouring pixels have similat motion\\n-> The optical glow methods in OpenCV will first take in a given set of points (user supplied) and a frame\\n-> Then it will attempt to find those points in the next frame\\n-> It is up to the user to supply the points to track\\n-> It uses LUCAS-KANADE(p: kana-de) function. The Lucas-Kandde function computes optical flow of a sparse \\n   feature set(tracks the specified points only; not all points in the video)\\n-> Gunner Farneback's algorithm(also built into OpenCV) uses Dense Optical flow(calclulates flow of all points in an image); it will color points black if no flow \\n   is detected\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "-> Optical flow is the pattern of apparent motion if image objects between two consecutive frames caused by the movement of object or camera\n",
    "-> (Our) Optical Flow analysis here have few assumptions:\n",
    "    --> The pixel intentsities of an object do not change between consecutive feames(Ex you're not trying to track a blinking bulb)\n",
    "    --> Neighbouring pixels have similat motion\n",
    "-> The optical glow methods in OpenCV will first take in a given set of points (user supplied) and a frame\n",
    "-> Then it will attempt to find those points in the next frame\n",
    "-> It is up to the user to supply the points to track\n",
    "-> It uses LUCAS-KANADE(p: kana-de) function. The Lucas-Kandde function computes optical flow of a sparse \n",
    "   feature set(tracks the specified points only; not all points in the video)\n",
    "-> Gunner Farneback's algorithm(also built into OpenCV) uses Dense Optical flow(calclulates flow of all points in an image); it will color points black if no flow \n",
    "   is detected\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' \n",
    "-> Window size parameter:\n",
    "   --> smaller window:more sensitive to noise; miss larger movement\n",
    "   --> larger window: miss smaller movements\n",
    "-> max Level : Lucas Kanade algorithm can be used with Image Pyramid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking1.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture/read video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('../../data/raw/Computer-Vision-with-Python_Joseph_Portilla/DATA/moving_car2.mov')\n",
    "\n",
    "return_value, previous_frame = capture.read()\n",
    "\n",
    "plt.imshow(previous_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create Markers(using goodFeaturesToTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Docstring:\n",
    "goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) -> corners\n",
    ".   @brief Determines strong corners on an image.\n",
    ".   The function finds the most prominent corners in the image or in the specified image region \n",
    ".   -   Function calculates the corner quality measure at every source image pixel using the #cornerMinEigenVal or #cornerHarris .\n",
    ".   -   Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are retained).\n",
    ".   -   The corners with the minimal eigenvalue less than \\f$\\texttt{qualityLevel} \\cdot \\max_{x,y} qualityMeasureMap(x,y)\\f$ are rejected.\n",
    ".   -   The remaining corners are sorted by the quality measure in the descending order.\n",
    ".   -   Function throws away each corner for which there is a stronger corner at a distance less than  maxDistance.\n",
    ".   \n",
    ".   The function can be used to initialize a point-based tracker of an object.\n",
    ".  \n",
    ".   @param image Input 8-bit or floating-point 32-bit, single-channel image.\n",
    ".   @param corners Output vector of detected corners.\n",
    ".   @param maxCorners Maximum number of corners to return. If there are more corners than are found,\n",
    ".   the strongest of them is returned. `maxCorners <= 0` implies that no limit on the maximum is set\n",
    ".   and all detected corners are returned.\n",
    ".   @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The\n",
    ".   parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue\n",
    ".   (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the\n",
    ".   quality measure less than the product are rejected. For example, if the best corner has the\n",
    ".   quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure\n",
    ".   less than 15 are rejected.\n",
    ".   @param minDistance Minimum possible Euclidean distance between the returned corners.\n",
    ".   @param mask Optional region of interest. If the image is not empty (it needs to have the type\n",
    ".   CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.\n",
    ".   @param blockSize Size of an average block for computing a derivative covariation matrix over each\n",
    ".   pixel neighborhood. See cornerEigenValsAndVecs .\n",
    ".   @param useHarrisDetector Parameter indicating whether to use a Harris detector (see #cornerHarris)\n",
    ".   or #cornerMinEigenVal.\n",
    ".   @param k Free parameter of the Harris detector.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using goodFeaturesToTrack to get the markers to track\n",
    "previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "corner_track_params = dict(maxCorners=8, qualityLevel=0.1,minDistance=20, blockSize=10)\n",
    "\n",
    "previous_points = cv2.goodFeaturesToTrack(previous_frame_gray, mask=None, **corner_track_params)\n",
    "previous_points_integer = np.int0(previous_points)\n",
    "\n",
    "# Display the markers\n",
    "previous_frame_copy = previous_frame.copy()\n",
    "for i in previous_points_integer:\n",
    "    x, y = i.ravel() # flatten the array\n",
    "    #print(x,y)\n",
    "    cv2.circle(previous_frame_copy, (x,y), radius=10, color=[255,0,0], thickness=-1)\n",
    "plt.imshow(previous_frame_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lucas Kanade algorithm (sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for Lucas Kanade Optical Flow\n",
    "\n",
    "Detect the motion of specific points or the aggregated motion of regions by modifying the winSize argument. This determines the integration window size. Small windows are more sensitive to noise and may miss larger motions. Large windows will “survive” an occlusion.\n",
    "\n",
    "The integration appears smoother with the larger window size.\n",
    "\n",
    "criteria has two here - the max number (10 above) of iterations and epsilon (0.03 above). More iterations means a more exhaustive search, and a smaller epsilon finishes earlier. These are primarily useful in exchanging speed vs accuracy, but mainly stay the same.\n",
    "\n",
    "When maxLevel is 0, it is the same algorithm without using pyramids (ie, calcOpticalFlowLK). Pyramids allow finding optical flow at various resolutions of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "LUCAS-KANADE \n",
    "Docstring:\n",
    "calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts[, status[, err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]) -> nextPts, status, err\n",
    ".   @brief Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.\n",
    ".   \n",
    ".   @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n",
    ".   @param nextImg second input image or pyramid of the same size and the same type as prevImg.\n",
    ".   @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be\n",
    "            single-precision floating-point numbers.\n",
    ".   @param nextPts output vector of 2D points (with single-precision floating-point coordinates)\n",
    "            containing the calculated new positions of input features in the second image; when\n",
    "            OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n",
    ".   @param status output status vector (of unsigned chars); each element of the vector is set to 1 if\n",
    "            the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    ".   @param err output vector of errors; each element of the vector is set to an error for the\n",
    "            corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't\n",
    "            found then the error is not defined (use the status parameter to find such cases).\n",
    ".   @param winSize size of the search window at each pyramid level.\n",
    ".   @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single\n",
    "        level), if set to 1, two levels are used, and so on; if pyramids are passed to input then\n",
    "        algorithm will use as many levels as pyramids have but no more than maxLevel.\n",
    ".   @param criteria parameter, specifying the termination criteria of the iterative search algorithm\n",
    "        (after the specified maximum number of iterations criteria.maxCount or when the search window\n",
    "        moves by less than criteria.epsilon.\n",
    ".   @param flags operation flags:\n",
    ".   -   **OPTFLOW_USE_INITIAL_FLOW** uses initial estimations, stored in nextPts; if the flag is\n",
    "          not set, then prevPts is copied to nextPts and is considered the initial estimate.\n",
    ".   -   **OPTFLOW_LK_GET_MIN_EIGENVALS** use minimum eigen values as an error measure (see\n",
    "           minEigThreshold description); if the flag is not set, then L1 distance between patches\n",
    "           around the original and a moved point, divided by number of pixels in a window, is used as a error measure.\n",
    ".   @param minEigThreshold the algorithm calculates the minimum eigen value of a 2x2 normal matrix of\n",
    "           optical flow equations (this matrix is called a spatial gradient matrix in @cite Bouguet00), divided\n",
    "           by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding\n",
    "           feature is filtered out and its flow is not processed, so it allows to remove bad points and get a performance boost.\n",
    "\n",
    ".   The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See\n",
    ".   @cite Bouguet00 . The function is parallelized with the TBB library.\n",
    ".   \n",
    ".   @note\n",
    ".   \n",
    ".   -   An example using the Lucas-Kanade optical flow algorithm can be found at\n",
    ".   opencv_source_code/samples/cpp/lkdemo.cpp\n",
    ".   -   (Python) An example using the Lucas-Kanade optical flow algorithm can be found at\n",
    ".   opencv_source_code/samples/python/lk_track.py\n",
    ".   -   (Python) An example using the Lucas-Kanade tracker for homography matching can be found at\n",
    ".   opencv_source_code/samples/python/lk_homography.pyC\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.TERM_CRITERIA_COUNT is the number of iterations (larger value -> more iterations -> more time); here we use 10\n",
    "# cv2.TERM_CRITERIA_EPS is the epsolon(?) values; here we use 0.03 (smaller value -> less time)\n",
    "# These 2 parameters decide speed v/s accuracy\n",
    "\n",
    "#corner_track_params = dict(maxCorners=10, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "lucas_kanade_params = dict(winSize=(200,200), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking2.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(previous_frame)\n",
    "previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #frame_gray = frame.copy()\n",
    "    \n",
    "    next_points, status, error = cv2.calcOpticalFlowPyrLK(previous_frame_gray, frame_gray, prevPts=previous_points, nextPts=None, **lucas_kanade_params)\n",
    "    \n",
    "    good_next_points = next_points[status == 1]\n",
    "    good_prev_points = previous_points[status == 1]\n",
    "    \n",
    "    for i, (new, prev) in enumerate(zip(good_next_points, good_prev_points)):\n",
    "        x_new,  y_new = new.ravel()\n",
    "        x_prev, y_prev = prev.ravel()\n",
    "        \n",
    "        # tracking lines for the markers\n",
    "        mask = cv2.line(mask, (x_new, y_new), (x_prev, y_prev), (0,255,0), 3)\n",
    "        frame = cv2.circle(frame, (x_new, y_new), 8, (0,0,255), -1)\n",
    "        \n",
    "    image = cv2.add(frame, mask)\n",
    "    cv2.imshow('tracking', image) \n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "    # replace previous_frame_gray with new frame_gray, previous_points with good_next_points for next iteration\n",
    "    previous_frame_gray = frame_gray.copy()\n",
    "    previous_points = good_next_points.reshape(-1,1,2)\n",
    "    \n",
    "    \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "# OUTPUT screen shot is shown below(computer_vision7_object_tracking2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking2.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(frame_gray), np.max(frame_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image \n",
    "from datetime import datetime\n",
    "from matplotlib import cm\n",
    "\n",
    "capture = cv2.VideoCapture('../../data/raw/Computer-Vision-with-Python_Joseph_Portilla/DATA/moving_car3.mov')\n",
    "return_value, previous_frame = capture.read()\n",
    "#plt.imshow(previous_frame)\n",
    "\n",
    "# Using goodFeaturesToTrack to get the markers to track\n",
    "previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "corner_track_params = dict(maxCorners=8, qualityLevel=0.1,minDistance=20, blockSize=10)\n",
    "\n",
    "previous_points = cv2.goodFeaturesToTrack(previous_frame_gray, mask=None, **corner_track_params)\n",
    "previous_points_integer = np.int0(previous_points)\n",
    "\n",
    "# Display the markers\n",
    "previous_frame_copy = previous_frame.copy()\n",
    "for i in previous_points_integer:\n",
    "    x, y = i.ravel() # flatten the array\n",
    "    #print(x,y)\n",
    "    cv2.circle(previous_frame_copy, (x,y), radius=10, color=[255,0,0], thickness=-1)\n",
    "plt.imshow(previous_frame_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas_kanade_params = dict(winSize=(200,200), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "mask = np.zeros_like(previous_frame)\n",
    "previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    if not return_value:\n",
    "        break\n",
    "        \n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #frame_gray = frame.copy()\n",
    "    \n",
    "    next_points, status, error = cv2.calcOpticalFlowPyrLK(previous_frame_gray, frame_gray, prevPts=previous_points, nextPts=None, **lucas_kanade_params)\n",
    "    \n",
    "    good_next_points = next_points[status == 1]\n",
    "    good_prev_points = previous_points[status == 1]\n",
    "    \n",
    "    for i, (new, prev) in enumerate(zip(good_next_points, good_prev_points)):\n",
    "        x_new,  y_new = new.ravel()\n",
    "        x_prev, y_prev = prev.ravel()\n",
    "        \n",
    "        # tracking lines for the markers\n",
    "        #mask = cv2.line(mask, (x_new, y_new), (x_prev, y_prev), (0,255,0), 1)\n",
    "        frame = cv2.circle(frame, (x_new, y_new), 4, tuple(np.array(cm.tab10(0)[:3]) * 255), -1)\n",
    "        \n",
    "    image = cv2.add(frame, mask)\n",
    "    cv2.imshow('tracking', image) \n",
    "    filename= datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    plt.imsave(filename, image)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        print('got here')\n",
    "        break\n",
    "\n",
    "    # replace previous_frame_gray with new frame_gray, previous_points with good_next_points for next iteration\n",
    "    previous_frame_gray = frame_gray.copy()\n",
    "    previous_points = good_next_points.reshape(-1,1,2)\n",
    "    \n",
    "    \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "# OUTPUT screen shot is shown below(computer_vision7_object_tracking3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(name, image):\n",
    "    cv2.startWindowThread()\n",
    "    cv2.imshow(name,image)\n",
    "    # cv2.startWindowThread()\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "    for i in range (1,10):\n",
    "        cv2.waitKey(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking3.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Optical Flow in OpenCV\n",
    "\n",
    "calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags) -> flow\n",
    "\n",
    "This function computes a dense optical flow using the Gunnar Farneback's algorithm.\n",
    "\n",
    "Here are the parameters for the function and what they represent:\n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* prev first 8-bit single-channel input image.\n",
    "* next second input image of the same size and the same type as prev.\n",
    "* flow computed flow image that has the same size as prev and type CV_32FC2.\n",
    "* pyr_scale parameter, specifying the image scale (\\<1) to build pyramids for each image\n",
    "    * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.\n",
    "    \n",
    "* levels number of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.\n",
    "* winsize averaging window size\n",
    "    * larger values increase the algorithm robustness to image\n",
    "* noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
    "* iterations number of iterations the algorithm does at each pyramid level.\n",
    "* poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel\n",
    "    * larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
    "* poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Docstring:\n",
    "calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags) -> flow\n",
    ".   @brief Computes a dense optical flow using the Gunnar Farneback's algorithm.\n",
    ".   \n",
    ".   @param prev first 8-bit single-channel input image.\n",
    ".   @param next second input image of the same size and the same type as prev.\n",
    ".   @param flow computed flow image that has the same size as prev and type CV_32FC2.\n",
    ".   @param pyr_scale parameter, specifying the image scale (\\<1) to build pyramids for each image;\n",
    ".   pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous\n",
    ".   one.\n",
    ".   @param levels number of pyramid layers including the initial image; levels=1 means that no extra\n",
    ".   layers are created and only the original images are used.\n",
    ".   @param winsize averaging window size; larger values increase the algorithm robustness to image\n",
    ".   noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
    ".   @param iterations number of iterations the algorithm does at each pyramid level.\n",
    ".   @param poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel;\n",
    ".   larger values mean that the image will be approximated with smoother surfaces, yielding more\n",
    ".   robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
    ".   @param poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a\n",
    ".   basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a\n",
    ".   good value would be poly_sigma=1.5.\n",
    ".   @param flags operation flags that can be a combination of the following:\n",
    ".   -   **OPTFLOW_USE_INITIAL_FLOW** uses the input flow as an initial flow approximation.\n",
    ".   -   **OPTFLOW_FARNEBACK_GAUSSIAN** uses the Gaussian \\f$\\texttt{winsize}\\times\\texttt{winsize}\\f$\n",
    ".   filter instead of a box filter of the same size for optical flow estimation; usually, this\n",
    ".   option gives z more accurate flow than with a box filter, at the cost of lower speed;\n",
    ".   normally, winsize for a Gaussian window should be set to a larger value to achieve the same\n",
    ".   level of robustness.\n",
    ".   \n",
    ".   The function finds an optical flow for each prev pixel using the @cite Farneback2003 algorithm so that\n",
    ".   \n",
    ".   \\f[\\texttt{prev} (y,x)  \\sim \\texttt{next} ( y + \\texttt{flow} (y,x)[1],  x + \\texttt{flow} (y,x)[0])\\f]\n",
    ".   \n",
    ".   @note\n",
    ".   \n",
    ".   -   An example using the optical flow algorithm described by Gunnar Farneback can be found at\n",
    ".   opencv_source_code/samples/cpp/fback.cpp\n",
    ".   -   (Python) An example using the optical flow algorithm described by Gunnar Farneback can be\n",
    ".   found at opencv_source_code/samples/python/opt_flow.py\n",
    "Type:      builtin_function_or_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "# Capture the frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "# Get gray scale image of first frame and make a mask in HSV color\n",
    "prvsImg = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv_mask = np.zeros_like(frame1)\n",
    "hsv_mask[:,:,1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    nextImg = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Check out the markdown text above for a break down of these paramters, most of these are just suggested defaults\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvsImg,nextImg, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    \n",
    "    # Color the channels based on the angle of travel\n",
    "    # Pay close attention to your video, the path of the direction of flow will determine color!\n",
    "    mag, ang = cv2.cartToPolar(flow[:,:,0], flow[:,:,1],angleInDegrees=True)\n",
    "    hsv_mask[:,:,0] = ang/2\n",
    "    hsv_mask[:,:,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert back to BGR to show with imshow from cv\n",
    "    bgr = cv2.cvtColor(hsv_mask,cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow('frame2',bgr)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # Set the Previous image as the next iamge for the loop\n",
    "    prvsImg = nextImg\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT shown below(computer_vision7_object_tracking6)\n",
    "# Different colors when the object moves in different direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking6.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTES: Below images explain how vector flow info is converted into hue,saturation, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking4.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking5.png', height=100, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEANSHIFT Alogorithm(similar to K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meanshift alogorithm is similar to k-means clustering, except that meanshift decides on the k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Imagine we have a set of points and we wanted to assign them to clusters\n",
    "2) We take all the data points and stack red and blue points on them(you cannot see red points underneath) (Initial state)\n",
    "3) The direction to the closest cluster centroid is determined by where most of the points nearby are at(weighted mean is calculated)\n",
    "4) So each iteration, each blue point will move closer to where the most points are at(which is or will lead to -> cluster centers)\n",
    "5) The red and blue data points overlap completely in the initial state (before the meanshift algorithm starts)\n",
    "6) At the end of iteration 1, all the blue points move towards the clusters (here it appears there will be 3 or 4 clusters)\n",
    "7) At the end of iteration 2, both bottom clusters hae begun to reach convergence\n",
    "8) At the end of iteration 3, meanshift found 3 clusters\n",
    "9) After subsequent iterations, the cluster means have stopped moving\n",
    "10) All the clusters have converged and there is no more movement\n",
    "11) Meanshift has identified 3 clusters\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking7.png', height=600, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How meanshift algorithm is applied to object tracking:\n",
    "-> Meanshift can be given a target to track; it calculates the color histogram of the target area, and then keep sliding the tracking window\n",
    "   to the closest match(the cluster center)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking8.png', height=400, width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking9.png', height=350, width=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAMSHIFT (Continuously adapted Meanshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Meanshift won't change the window size if the target moves away or toward the camera. We can use \n",
    "CAMshift to update the size of the window\n",
    "\n",
    "1) First apply meanshift and perform the iterations of the meanshift\n",
    "2) Once meanshift converges, we calculate a new region of interest. We do that by updating the size of the actual region of interst \n",
    "   and we also calculate the orientation of the best fitting elipse to the new region of interest.\n",
    "3) Again, meanshift is applied with the new scaled search window in previous window location.\n",
    "4) This process is continued until some sort of required accuracy is met\n",
    "All this happends behind the scenes with OpenCV\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking10.png', height=400, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/UDEMY_computer_vision_Joseph_Portilla/computer_vision7_object_tracking11.png', height=150, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: videos 62, 63, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

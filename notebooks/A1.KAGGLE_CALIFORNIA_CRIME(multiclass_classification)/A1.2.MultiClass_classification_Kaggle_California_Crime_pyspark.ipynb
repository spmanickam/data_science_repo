{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    CLASSIFICATION\n",
    "    BINOMIAL                                    MULTINOMIAL                                                                   \n",
    "    Binomial logistic regression                (Multinomial)logistic regression \n",
    "    ?Decision tree classifier                   Random forest classifier\n",
    "    ?Multilayer perceptron classifier           Gradient-boosted tree classifier\n",
    "    Linear Support Vector Machine               One-vs-Rest classifier\n",
    "    Linear Support Vector Machine               Naive Bayes                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTICLASS CLASSIFICAION(Kaggle_California_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MULTICLASS-CLASSIFICAION(Kaggle_California_crime)\" data-toc-modified-id=\"MULTICLASS-CLASSIFICAION(Kaggle_California_crime)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MULTICLASS CLASSIFICAION(Kaggle_California_crime)</a></span><ul class=\"toc-item\"><li><span><a href=\"#LOGISTIC-REGRESSION-(classification)\" data-toc-modified-id=\"LOGISTIC-REGRESSION-(classification)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>LOGISTIC REGRESSION (classification)</a></span><ul class=\"toc-item\"><li><span><a href=\"#EVALUATION\" data-toc-modified-id=\"EVALUATION-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>EVALUATION</a></span></li><li><span><a href=\"#PREDICTION-CONVERTED-BACK-TO-LABEL\" data-toc-modified-id=\"PREDICTION-CONVERTED-BACK-TO-LABEL-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>PREDICTION CONVERTED BACK TO LABEL</a></span></li><li><span><a href=\"#CROSS-VALIDATION\" data-toc-modified-id=\"CROSS-VALIDATION-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>CROSS VALIDATION</a></span></li><li><span><a href=\"#PRINT-PARAMS\" data-toc-modified-id=\"PRINT-PARAMS-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>PRINT PARAMS</a></span></li></ul></li><li><span><a href=\"#NAIVE-BAYES-(classification)\" data-toc-modified-id=\"NAIVE-BAYES-(classification)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>NAIVE BAYES (classification)</a></span></li><li><span><a href=\"#RANDOM-FOREST-(classification)\" data-toc-modified-id=\"RANDOM-FOREST-(classification)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>RANDOM FOREST (classification)</a></span><ul class=\"toc-item\"><li><span><a href=\"#PRINT-PARAMS\" data-toc-modified-id=\"PRINT-PARAMS-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>PRINT PARAMS</a></span></li></ul></li><li><span><a href=\"#NOTE:-LinearSVC-supports-binary-classification-only???\" data-toc-modified-id=\"NOTE:-LinearSVC-supports-binary-classification-only???-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>NOTE: LinearSVC supports binary classification only???</a></span></li><li><span><a href=\"#One-vs-Rest-(classification)\" data-toc-modified-id=\"One-vs-Rest-(classification)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>One-vs-Rest (classification)</a></span></li><li><span><a href=\"#GRADIENT-BOOSTED-TREE\" data-toc-modified-id=\"GRADIENT-BOOSTED-TREE-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>GRADIENT-BOOSTED TREE</a></span></li><li><span><a href=\"#MULTILAYER-PERCEPTRON\" data-toc-modified-id=\"MULTILAYER-PERCEPTRON-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>MULTILAYER PERCEPTRON</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#EXAMPLE\" data-toc-modified-id=\"EXAMPLE-1.7.0.1\"><span class=\"toc-item-num\">1.7.0.1&nbsp;&nbsp;</span>EXAMPLE</a></span></li></ul></li><li><span><a href=\"#trainingData-split\" data-toc-modified-id=\"trainingData-split-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>trainingData split</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MULTICLASS CLASSIFICAION</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10f20ccc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MULTICLASS CLASSIFICAION\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file= '/Users/pinky/Downloads/DATA/Kaggle/california_crime/train.csv'\n",
    "test_data_file= '/Users/pinky/Downloads/DATA/Kaggle/california_crime/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = spark.read.format('csv').option('header','true').load(train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                  X|                 Y|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|2015-05-13 23:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|  -122.438737622757|37.771541172057795|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF2 = trainDF.select('Category','Descript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      Category|            Descript|\n",
      "+--------------+--------------------+\n",
      "|      WARRANTS|      WARRANT ARREST|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Category      |count |\n",
      "+--------------+------+\n",
      "|LARCENY/THEFT |174900|\n",
      "|OTHER OFFENSES|126182|\n",
      "|NON-CRIMINAL  |92304 |\n",
      "|ASSAULT       |76876 |\n",
      "|DRUG/NARCOTIC |53971 |\n",
      "+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#distinct categories \n",
    "trainDF2.groupby('Category').count().sort('count', ascending=False).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-----+\n",
      "|Descript                             |count|\n",
      "+-------------------------------------+-----+\n",
      "|GRAND THEFT FROM LOCKED AUTO         |60022|\n",
      "|LOST PROPERTY                        |31729|\n",
      "|BATTERY                              |27441|\n",
      "|STOLEN AUTOMOBILE                    |26897|\n",
      "|DRIVERS LICENSE, SUSPENDED OR REVOKED|26839|\n",
      "+-------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF2.groupby('Descript').count().sort('count', ascending=False).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline: Tokenizer --> StopWordsRemover --> CountVectorizer\n",
    "# StringIndexer for label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stop_words = StopWordsRemover().getStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(default_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = default_stop_words.copy()\n",
    "new_stop_words.append('')\n",
    "type(new_stop_words)\n",
    "new_stop_words.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would',\n",
       " '',\n",
       " '-']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stop_words[len(new_stop_words)-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='Descript', outputCol='words')\n",
    "stopwordsremover = StopWordsRemover(inputCol='words', outputCol='words_cleansed', stopWords=new_stop_words)\n",
    "vectorizer = CountVectorizer(inputCol='words_cleansed', outputCol='features')\n",
    "\n",
    "label_stringIndexer = StringIndexer(inputCol='Category', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwordsremover, vectorizer, label_stringIndexer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranformedDF = pipeline.fit(trainDF2).transform(trainDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tranformedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Category|            Descript|               words|      words_cleansed|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(1055,[11,23],[1....|  7.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(1055,[8,11,26],[...|  1.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(1055,[8,11,26],[...|  1.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, lo...|(1055,[0,1,2,3],[...|  0.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, lo...|(1055,[0,1,2,3],[...|  0.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, un...|(1055,[0,1,2,92],...|  0.0|\n",
      "| VEHICLE THEFT|   STOLEN AUTOMOBILE|[stolen, automobile]|[stolen, automobile]|(1055,[7,18],[1.0...|  5.0|\n",
      "| VEHICLE THEFT|   STOLEN AUTOMOBILE|[stolen, automobile]|[stolen, automobile]|(1055,[7,18],[1.0...|  5.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, lo...|(1055,[0,1,2,3],[...|  0.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, lo...|(1055,[0,1,2,3],[...|  0.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tranformedDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Category='FRAUD'),\n",
       " Row(Category='SUICIDE'),\n",
       " Row(Category='SEX OFFENSES FORCIBLE'),\n",
       " Row(Category='LIQUOR LAWS'),\n",
       " Row(Category='SECONDARY CODES'),\n",
       " Row(Category='FAMILY OFFENSES'),\n",
       " Row(Category='MISSING PERSON'),\n",
       " Row(Category='OTHER OFFENSES'),\n",
       " Row(Category='DRIVING UNDER THE INFLUENCE'),\n",
       " Row(Category='WARRANTS'),\n",
       " Row(Category='ARSON'),\n",
       " Row(Category='SEX OFFENSES NON FORCIBLE'),\n",
       " Row(Category='FORGERY/COUNTERFEITING'),\n",
       " Row(Category='GAMBLING'),\n",
       " Row(Category='BRIBERY'),\n",
       " Row(Category='ASSAULT'),\n",
       " Row(Category='DRUNKENNESS'),\n",
       " Row(Category='EXTORTION'),\n",
       " Row(Category='TREA'),\n",
       " Row(Category='WEAPON LAWS'),\n",
       " Row(Category='LOITERING'),\n",
       " Row(Category='SUSPICIOUS OCC'),\n",
       " Row(Category='ROBBERY'),\n",
       " Row(Category='PROSTITUTION'),\n",
       " Row(Category='EMBEZZLEMENT'),\n",
       " Row(Category='BAD CHECKS'),\n",
       " Row(Category='DISORDERLY CONDUCT'),\n",
       " Row(Category='RUNAWAY'),\n",
       " Row(Category='RECOVERED VEHICLE'),\n",
       " Row(Category='VANDALISM'),\n",
       " Row(Category='DRUG/NARCOTIC'),\n",
       " Row(Category='PORNOGRAPHY/OBSCENE MAT'),\n",
       " Row(Category='TRESPASS'),\n",
       " Row(Category='VEHICLE THEFT'),\n",
       " Row(Category='NON-CRIMINAL'),\n",
       " Row(Category='STOLEN PROPERTY'),\n",
       " Row(Category='LARCENY/THEFT'),\n",
       " Row(Category='KIDNAPPING'),\n",
       " Row(Category='BURGLARY')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranformedDF.select('Category').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|words_cleansed                           |\n",
      "+-----------------------------------------+\n",
      "|[abandonment, child]                     |\n",
      "|[abortion]                               |\n",
      "|[access, card, information,, publication]|\n",
      "|[access, card, information,, theft]      |\n",
      "|[accidental, burns]                      |\n",
      "+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tranformedDF.select('words_cleansed').distinct().sort('words_cleansed').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|words_cleansed                              |\n",
      "+--------------------------------------------+\n",
      "|[youth, court]                              |\n",
      "|[willful, cruelty, child]                   |\n",
      "|[wearing, mask, disguise, unlawful, purpose]|\n",
      "|[wearing, apparel, opposite, sex, deceive]  |\n",
      "|[weapons, possession, juvenile, suspect]    |\n",
      "+--------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tranformedDF.select('words_cleansed').distinct().sort('words_cleansed', ascending=False).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.params\n",
    "# tokenizer.params\n",
    "# stopwordsremover.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do']\n"
     ]
    }
   ],
   "source": [
    "print(stopwordsremover.getStopWords()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = tranformedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614478, 263571)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logisticRegression = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = logisticRegression.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lrModel.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Category',\n",
       " 'Descript',\n",
       " 'words',\n",
       " 'words_cleansed',\n",
       " 'features',\n",
       " 'label',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "| 27.0|       0.0|\n",
      "| 27.0|       0.0|\n",
      "| 27.0|       0.0|\n",
      "| 27.0|       0.0|\n",
      "| 27.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(['label', 'prediction']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+--------------+------------------+-----+--------------------+--------------------+----------+\n",
      "|Category|Descript|  words|words_cleansed|          features|label|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------+--------------+------------------+-----+--------------------+--------------------+----------+\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[3.02305577241817...|[0.12781878150593...|       0.0|\n",
      "+--------+--------+-------+--------------+------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "| 38.0|    2|\n",
      "| 37.0|    6|\n",
      "| 36.0|   39|\n",
      "| 35.0|   48|\n",
      "| 34.0|   69|\n",
      "| 33.0|   86|\n",
      "| 32.0|  126|\n",
      "| 30.0|  146|\n",
      "| 31.0|  147|\n",
      "| 29.0|  330|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('label').count().sort('count').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Category|count|\n",
      "+--------------------+-----+\n",
      "|                TREA|    2|\n",
      "|PORNOGRAPHY/OBSCE...|    6|\n",
      "|            GAMBLING|   39|\n",
      "|SEX OFFENSES NON ...|   48|\n",
      "|           EXTORTION|   69|\n",
      "|             BRIBERY|   86|\n",
      "|          BAD CHECKS|  126|\n",
      "|             SUICIDE|  146|\n",
      "|     FAMILY OFFENSES|  147|\n",
      "|        EMBEZZLEMENT|  330|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('Category').count().sort('count').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrModel_accuracy: 0.980403762174139\n",
      "lrModel_weightedPrecision: 0.97492545179755\n",
      "lrModel_weightedPrecision: 0.97492545179755\n",
      "lrModel_weightedRecall: 0.9804037621741389\n",
      "lrModel_f1: 0.9748541010853148\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "mEvaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "lrModel_accuracy = mEvaluator.evaluate(predictions)\n",
    "print('lrModel_accuracy:', lrModel_accuracy)\n",
    "#O: 0.974708090565633   # The accuracy is excellent!\n",
    "#O2:0.980403762174139\n",
    "#O3: lrModel_accuracy: 0.980403762174139 \n",
    "\n",
    "mEvaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedPrecision')\n",
    "lrModel_weightedPrecision = mEvaluator.evaluate(predictions)\n",
    "print('lrModel_weightedPrecision:' , lrModel_weightedPrecision)\n",
    "#O: lrModel_weightedPrecision: 0.97492545179755\n",
    "\n",
    "mEvaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='weightedRecall')\n",
    "lrModel_weightedRecall = mEvaluator.evaluate(predictions)\n",
    "print('lrModel_weightedRecall:' , lrModel_weightedRecall)\n",
    "lrModel_weightedRecall: 0.9804037621741389\n",
    "    \n",
    "mEvaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "lrModel_f1 = mEvaluator.evaluate(predictions)\n",
    "print('lrModel_f1:' , lrModel_f1)\n",
    "lrModel_f1: 0.9748541010853148\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PREDICTION CONVERTED BACK TO LABEL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label_and_preductions_only = predictions.select('Category', 'label','prediction')\n",
    "label_list = 'FRAUD', 'SUICIDE', 'SEX OFFENSES FORCIBLE', 'LIQUOR LAWS', 'SECONDARY CODES', 'FAMILY OFFENSES', 'MISSING PERSON', 'OTHER OFFENSES', 'DRIVING UNDER THE INFLUENCE', 'WARRANTS', 'ARSON', 'SEX OFFENSES NON FORCIBLE', 'FORGERY/COUNTERFEITING', 'GAMBLING', 'BRIBERY', 'ASSAULT', 'DRUNKENNESS', 'EXTORTION', 'TREA', 'WEAPON LAWS', 'LOITERING', 'SUSPICIOUS OCC', 'ROBBERY', 'PROSTITUTION', 'EMBEZZLEMENT', 'BAD CHECKS', 'DISORDERLY CONDUCT', 'RUNAWAY', 'RECOVERED VEHICLE', 'VANDALISM', 'DRUG/NARCOTIC', 'PORNOGRAPHY/OBSCENE MAT', 'TRESPASS', 'VEHICLE THEFT', 'NON-CRIMINAL', 'STOLEN PROPERTY', 'LARCENY/THEFT', 'KIDNAPPING','BURGLARY'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "indexToStringTransformer = IndexToString(inputCol='prediction', outputCol='prediction_Converted', labels=label_list) \n",
    "result = indexToStringTransformer.transform(label_and_preductions_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.groupBy('Category','prediction_Converted').count().sort('count').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.groupBy('Category','prediction_Converted').sum().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.groupBy('Category').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.groupBy('prediction_Converted').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.groupBy('label','prediction').count().sort('prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.groupBy('Category','prediction_Converted').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(logisticRegression.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(logisticRegression.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=logisticRegression, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(trainingData)\n",
    "cv_predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+--------------+------------------+-----+--------------------+--------------------+----------+\n",
      "|Category|Descript|  words|words_cleansed|          features|label|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------+--------------+------------------+-----+--------------------+--------------------+----------+\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[2.77053080296224...|[0.07351631415968...|      27.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[2.77053080296224...|[0.07351631415968...|      27.0|\n",
      "|   ARSON|   ARSON|[arson]|       [arson]|(1055,[196],[1.0])| 27.0|[2.77053080296224...|[0.07351631415968...|      27.0|\n",
      "+--------+--------+-------+--------------+------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = cv_evaluator.evaluate(cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956178790534619"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result  \n",
    "#O:0.9952851156305446 #The performance improved.\n",
    "#O2: 0.9956178790534619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_45ff9fe1573f1926e73e"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'), 2), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'), 0.0), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'), 'auto'), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='featuresCol', doc='features column name'), 'features'), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='fitIntercept', doc='whether to fit an intercept term'), True), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='labelCol', doc='label column name'), 'label'), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='maxIter', doc='maximum number of iterations (>= 0)'), 20), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='predictionCol', doc='prediction column name'), 'prediction'), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'), 'probability'), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'), 'rawPrediction'), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='regParam', doc='regularization parameter (>= 0)'), 0.1), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='standardization', doc='whether to standardize the training features before fitting the model'), True), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'), 0.5), (Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'), 1e-06)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel_params = bestModel.extractParamMap()\n",
    "bestModel_params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 0.0, 'auto', 'features', True, 'label', 20, 'prediction', 'probability', 'rawPrediction', 0.1, True, 0.5, 1e-06])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel_params.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.getParam('elasticNetParam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='LogisticRegression_45ff9fe1573f1926e73e', name='regParam', doc='regularization parameter (>= 0)')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.getParam('regParam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINT PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth : 2\n",
      "elasticNetParam : 0.0\n",
      "family : auto\n",
      "featuresCol : features\n",
      "fitIntercept : True\n",
      "labelCol : label\n",
      "maxIter : 20\n",
      "predictionCol : prediction\n",
      "probabilityCol : probability\n",
      "rawPredictionCol : rawPrediction\n",
      "regParam : 0.1\n",
      "standardization : True\n",
      "threshold : 0.5\n",
      "tol : 1e-06\n"
     ]
    }
   ],
   "source": [
    "for key, value in bestModel.extractParamMap().items():\n",
    "    print (key.name, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956178790534619"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " betsModel_predictions = bestModel.transform(testData)\n",
    "#cv_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',metricName='accuracy')\n",
    "cv_evaluator.evaluate(betsModel_predictions)\n",
    "#O: 0.9952851156305446 # SAME RESULT as cvModel_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES (classification)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayesâ€™ theorem with strong (naive) independence assumptions between the features. The spark.ml implementation currently supports both multinomial naive Bayes and Bernoulli naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes = NaiveBayes()\n",
    "                             #featuresCol='features', \n",
    "                             #labelCol='label',\n",
    "                             #predictionCol='', \n",
    "                             #modelType='',  #\"multinomial\" (default) and \"bernoulli\"\n",
    "                             #probabilityCol='',\n",
    "                             #rawPredictionCol='',\n",
    "                             #smoothing='',  #smoothing parameter. Default is 1.0.\n",
    "                             #self='', \n",
    "                             #thresholds='', \n",
    "                             #weightCol='', #default: set or not empty; so all instances have weight=1\n",
    "                             #lambda='')   #Set the smoothing parameter. Default: 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayesModel = naiveBayes.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayesPredictions = naiveBayesModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naiveBayesPredictions.orderBy('probability').sort('probability').show(3)\n",
    "#naiveBayesPredictions.orderBy('probability').sort('probability', ascenging=False).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- words_cleansed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naiveBayesPredictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayesEvaluator = MulticlassClassificationEvaluator(predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9940657249526593"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naiveBayesEvaluator.evaluate(naiveBayesPredictions)\n",
    "# O: 0.993997341990219   # Good, but not better than LogisticRegression Cross validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestClassifier(numTrees=100, maxBins=32, maxDepth=4)\n",
    "#                                       numTrees='',  #(default = 20) \n",
    "                                                      #If 1, then no bootstrapping is used. \n",
    "#                                                     #If > 1, then bootstrapping is done\n",
    "                                                      #cannot use this property in GBT or \n",
    "                                                      #RF(TreeEnsembleParams) because maxIter controls this\n",
    "#                                       impurity='',  #Criterion used for information gain calculation (case-insensitive).\n",
    "                                                      #gini\" (recommended) or \"entropy\"\n",
    "#                                       maxBins='',   # (default = 32)\n",
    "                                                      #Maximum number of bins used for discretizing continuous features and for choosing \n",
    "                                                      #how to split on features at each node. More bins give \n",
    "                                                      #higher granularity. Must be >= 2 and >= number of categories \n",
    "                                                      #in any categorical feature. \n",
    "#                                       maxDepth='',  #(suggested value: 4)\n",
    "#                                       checkpointInterval='', #set checkpoint interval (>= 1) or disable checkpoint (-1). \n",
    "                                                               #Ex: 10 means that the cache will get checkpointed every 10 iterations.\n",
    "#                                       featureSubsetStrategy='', #(default = \"auto\")\n",
    "                                                                  #The number of features to consider for splits at each tree node\n",
    "                                                                  # auto | all | onethird | sqrt | log2 | n * number of features, with n = range(0.0,1.0)\n",
    "                                                                  # auto: 1 if numTress = 1; \n",
    "                                                                  # all :use all features\n",
    "                                                                  # onethird: use one third of the features\n",
    "                                                                  # sqrt: sqrt(number of features); recommended by Breiman manual for random forests\n",
    "                                                                  # log2: log2(number of features)\n",
    "\n",
    "#                                       maxMemoryInMB='',\n",
    "#                                       minInfoGain='', #(default = 0.0)\n",
    "                                                        #Minimum information gain for a split to be considered at a tree node. \n",
    "                                                        #Should be >= 0.0. \n",
    "#                                       minInstancesPerNode='', #(default = 1)\n",
    "                                                                #Minimum number of instances each child must have after split. \n",
    "                                                                #If a split causes the left or right child to have fewer than minInstancesPerNode, \n",
    "                                                                #the split will be discarded as invalid. Should be >= 1\n",
    "#                                       predictionCol='',\n",
    "#                                       seed='',\n",
    "#                                       subsamplingRate='', #Fraction of the training data used for learning \n",
    "                                                            #each decision tree, in range (0, 1]. (default = 1.0)\n",
    "#                                       featuresCol='',\n",
    "#                                       labelCol='',\n",
    "#                                       probabilityCol='',\n",
    "#                                       rawPredictionCol='',\n",
    "#                                       thresholds='')   # adjust the probability of predicting each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'randomForestModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-aa58d347ae57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomForestModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'randomForestModel' is not defined"
     ]
    }
   ],
   "source": [
    "type(randomForestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(randomForestModel) # works with DATABRICKS only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(display(randomForestModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel = randomForest.fit(trainingData)\n",
    "randomForestPredictions = randomForestModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestEvaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "randomForestEvaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a very good, robust and versatile method, however itâ€™s no mystery that for high-dimensional sparse data itâ€™s not a best choice.\n",
    "\n",
    "It is obvious that Logistic Regression will be our model in this experiment, with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val params = cvModel3Day.bestModel.asInstanceOf[PipelineModel].stages(2).asInstanceOf[GBTClassificationModel].extractParamMap()\n",
    "\n",
    "val depth = cvModel3Day.bestModel.asInstanceOf[PipelineModel].stages(2).asInstanceOf[GBTClassificationModel].getMaxDepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINT PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = randomForestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[lambda x: param.values(x) for x in params.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in randomForestModel.extractParamMap().items():\n",
    "    print(key.name, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = randomForestModel.getOrDefault('maxMemoryInMB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: LinearSVC supports binary classification only???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-Rest (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OneVsRest(classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrModel = ovr.fit(trainingData)\n",
    "ovr_predictions = ovrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrEvaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "ovrEvaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT-BOOSTED TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTILAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (MLPC) is a classifier based on the feedforward artificial neural network. MLPC consists of multiple layers of nodes. Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes map inputs to outputs by a linear combination of the inputs with the nodeâ€™s weights w and bias b and applying an activation function\n",
    "    The number of nodes N in the output layer corresponds to the number of classes.MLPC employs backpropagation for learning the model. We use the logistic loss function for optimization and L-BFGS as an optimization routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_layers = [4, 5, 4, 3]\n",
    "#mPerceptron = MultilayerPerceptronClassifier(layers=my_layers, blockSize=128, maxIter=100, seed=1234)\n",
    "trainer = MultilayerPerceptronClassifier(layers=[856, 5, 4, 39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPerceptronModel = trainer.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPerceptronPredictions = mPerceptronModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Category: string, Descript: string, words: array<string>, words_cleansed: array<string>, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mPerceptronPredictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mPerceptronPredictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mPerceptronEvaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "mPerceptronPredictions.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_parameter_grid_ = ParamGridBuilder().addGrid(mr.maxIter, [50, 200, 500])\\\n",
    "    .addGrid(lr.regParam, [0, 0.3, 1])\\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.3, 1]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "| features|prediction|\n",
      "+---------+----------+\n",
      "|[1.0,0.0]|       1.0|\n",
      "|[0.0,0.0]|       0.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([\n",
    "      (0.0, Vectors.dense([0.0, 0.0])),\n",
    "      (1.0, Vectors.dense([0.0, 1.0])),\n",
    "      (1.0, Vectors.dense([1.0, 0.0])),\n",
    "      (0.0, Vectors.dense([1.0, 1.0]))], [\"label\", \"features\"])\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[2, 2, 2], blockSize=1, seed=123)\n",
    "model = mlp.fit(df)\n",
    "testDF = spark.createDataFrame([\n",
    "      (Vectors.dense([1.0, 0.0]),),\n",
    "      (Vectors.dense([0.0, 0.0]),)], [\"features\"])\n",
    "model.transform(testDF).select(\"features\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- words_cleansed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|Category              |count |\n",
      "+----------------------+------+\n",
      "|LARCENY/THEFT         |122363|\n",
      "|OTHER OFFENSES        |88319 |\n",
      "|NON-CRIMINAL          |64260 |\n",
      "|ASSAULT               |53837 |\n",
      "|DRUG/NARCOTIC         |37826 |\n",
      "|VEHICLE THEFT         |37608 |\n",
      "|VANDALISM             |31385 |\n",
      "|WARRANTS              |29733 |\n",
      "|BURGLARY              |25771 |\n",
      "|SUSPICIOUS OCC        |22024 |\n",
      "|MISSING PERSON        |18214 |\n",
      "|ROBBERY               |16098 |\n",
      "|FRAUD                 |11631 |\n",
      "|FORGERY/COUNTERFEITING|7338  |\n",
      "|SECONDARY CODES       |6999  |\n",
      "|WEAPON LAWS           |5987  |\n",
      "|PROSTITUTION          |5253  |\n",
      "|TRESPASS              |5110  |\n",
      "|STOLEN PROPERTY       |3139  |\n",
      "|SEX OFFENSES FORCIBLE |3057  |\n",
      "+----------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.groupby('Category').count().sort('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainingData split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61479, 614478)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ERROR: trainingData.filter(trainingData['Category'] in ('PROSTITUTION','TRESPASS','TRESPASS'))\n",
    "trainingData2,_=trainingData.randomSplit([0.1, 0.9])\n",
    "trainingData2.count(),trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26457, 263571)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData2,_=testData.randomSplit([0.1, 0.9])\n",
    "testData2.count(),testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MultilayerPerceptronClassifier(layers=[856, 5, 4, 662])\n",
    "mPerceptronModel = trainer.fit(trainingData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Category: string, Descript: string, words: array<string>, words_cleansed: array<string>, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mPerceptronPredictions = mPerceptronModel.transform(testData2)\n",
    "mPerceptronPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Category', 'Descript', 'words', 'words_cleansed', 'features', 'label']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #trainingData2.select('Category').distinct().collect() #Row count:39\n",
    " trainingData2.select('Descript').distinct().count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mPerceptronPredictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
